"""
Scraper Service Main Application

Entry point –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π.

====== USAGE ======

# CLI commands:
python -m app.main scrape-all        # –ü–∞—Ä—Å–∏—Ç—å –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
python -m app.main scrape-rss        # –¢–æ–ª—å–∫–æ RSS
python -m app.main scrape-api        # –¢–æ–ª—å–∫–æ News API
python -m app.main scrape bbc        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
python -m app.main list-sources      # –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
python -m app.main test              # –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞

# Celery commands:
celery -A app.celery_app worker -B --loglevel=info
celery -A app.celery_app flower      # Monitoring UI
"""

import click
import logging
from typing import Optional
from rich.console import Console
from rich.table import Table
from rich import print as rprint

from app.config import settings, get_all_sources
from app.scrapers.rss_scraper import MultiFeedScraper
from app.scrapers.api_scraper import NewsAPIScraper
from app.tasks.scraping_tasks import (
    scrape_all_sources,
    scrape_rss_feeds,
    scrape_source
)


# Setup logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Rich console –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
console = Console()


# ===== CLI APPLICATION =====

@click.group()
@click.version_option(version=settings.APP_VERSION)
def cli():
    """Smart News Scraper Service CLI.
    
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    """
    pass


# ===== SCRAPING COMMANDS =====

@cli.command()
@click.option('--async', 'use_async', is_flag=True, help='–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Celery')
def scrape_all(use_async: bool):
    """–ü–∞—Ä—Å–∏—Ç—å –í–°–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π."""
    console.print("\n[bold blue]üï∑Ô∏è  Starting full scrape...[/bold blue]\n")
    
    if use_async:
        # Async —á–µ—Ä–µ–∑ Celery
        task = scrape_all_sources.delay()
        console.print(f"[green]‚úÖ Task started: {task.id}[/green]")
        console.print(f"[yellow]Status: {task.status}[/yellow]")
        
        # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        console.print("[yellow]Waiting for results...[/yellow]")
        stats = task.get(timeout=600)  # 10 –º–∏–Ω—É—Ç max
        
        display_scrape_stats(stats)
        
    else:
        # Sync –∑–∞–ø—É—Å–∫
        stats = scrape_all_sources()
        display_scrape_stats(stats)


@cli.command()
def scrape_rss():
    """–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ RSS –ª–µ–Ω—Ç—ã."""
    console.print("\n[bold blue]üì∞ Scraping RSS feeds...[/bold blue]\n")
    
    scraper = MultiFeedScraper()
    articles = scraper.scrape_all()
    
    console.print(f"[green]‚úÖ Scraped {len(articles)} articles from RSS[/green]\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    if articles:
        display_articles_table(articles[:10])


@cli.command()
def scrape_api():
    """–ü–∞—Ä—Å–∏—Ç—å —á–µ—Ä–µ–∑ News API."""
    if not settings.NEWS_API_KEY:
        console.print("[red]‚ùå NEWS_API_KEY not set![/red]")
        console.print("[yellow]Set it in .env file or environment variables[/yellow]")
        return
    
    console.print("\n[bold blue]üì° Scraping News API...[/bold blue]\n")
    
    scraper = NewsAPIScraper()
    articles = scraper.run()
    
    console.print(f"[green]‚úÖ Scraped {len(articles)} articles from News API[/green]\n")
    
    if articles:
        display_articles_table(articles[:10])


@cli.command()
@click.argument('source_name')
def scrape(source_name: str):
    """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫.
    
    Example: python -m app.main scrape bbc
    """
    console.print(f"\n[bold blue]üéØ Scraping {source_name}...[/bold blue]\n")
    
    task = scrape_source.delay(source_name)
    articles_dict = task.get(timeout=120)
    
    console.print(f"[green]‚úÖ Scraped {len(articles_dict)} articles[/green]\n")
    
    if articles_dict:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º dict –æ–±—Ä–∞—Ç–Ω–æ –≤ Article –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        from app.scrapers.base_scraper import Article
        articles = [
            Article(
                title=a['title'],
                url=a['url'],
                content=a.get('content', ''),
                source=a.get('source', source_name)
            )
            for a in articles_dict[:10]
        ]
        display_articles_table(articles)


# ===== INFO COMMANDS =====

@cli.command()
def list_sources():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    console.print("\n[bold blue]üìã Available News Sources[/bold blue]\n")
    
    # RSS Feeds
    console.print("[bold yellow]RSS Feeds:[/bold yellow]")
    for name, url in settings.RSS_FEEDS.items():
        console.print(f"  ‚Ä¢ {name:20} {url}")
    
    console.print()
    
    # News API
    console.print("[bold yellow]News API Sources:[/bold yellow]")
    for source in settings.NEWS_API_SOURCES:
        console.print(f"  ‚Ä¢ {source}")
    
    console.print()
    
    total = len(get_all_sources())
    console.print(f"[green]Total sources: {total}[/green]\n")


@cli.command()
def config_info():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é."""
    console.print("\n[bold blue]‚öôÔ∏è  Configuration[/bold blue]\n")
    
    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("Setting", style="cyan")
    table.add_column("Value", style="green")
    
    table.add_row("App Name", settings.APP_NAME)
    table.add_row("Version", settings.APP_VERSION)
    table.add_row("Debug", str(settings.DEBUG))
    table.add_row("Backend URL", settings.BACKEND_URL)
    table.add_row("ML Service URL", settings.ML_SERVICE_URL)
    table.add_row("Celery Broker", settings.CELERY_BROKER_URL)
    table.add_row("Scrape Interval", f"{settings.SCRAPE_INTERVAL_MINUTES} min")
    table.add_row("Min Article Length", f"{settings.MIN_ARTICLE_LENGTH} words")
    table.add_row("Allowed Languages", ", ".join(settings.ALLOWED_LANGUAGES))
    
    console.print(table)
    console.print()


# ===== TEST COMMANDS =====

@cli.command()
@click.option('--source', default='bbc', help='–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è —Ç–µ—Å—Ç–∞')
def test(source: str):
    """–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    console.print(f"\n[bold blue]üß™ Testing scraper: {source}[/bold blue]\n")
    
    try:
        # –ü—Ä–æ–±—É–µ–º RSS
        rss_url = settings.RSS_FEEDS.get(source)
        if rss_url:
            from app.scrapers.rss_scraper import RSSFeedScraper
            
            scraper = RSSFeedScraper(source, rss_url)
            articles = scraper.run()
            
            console.print(f"[green]‚úÖ RSS test successful: {len(articles)} articles[/green]")
            
            if articles:
                article = articles[0]
                console.print("\n[bold]Sample Article:[/bold]")
                console.print(f"Title: {article.title}")
                console.print(f"URL: {article.url}")
                console.print(f"Published: {article.published_at}")
                console.print(f"Content: {article.content[:200]}...")
        else:
            console.print(f"[red]‚ùå Source '{source}' not found in RSS feeds[/red]")
            
    except Exception as e:
        console.print(f"[red]‚ùå Test failed: {e}[/red]")
        logger.exception("Test error")


# ===== CELERY COMMANDS =====

@cli.command()
def worker():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å Celery worker."""
    console.print("\n[bold blue]üë∑ Starting Celery worker...[/bold blue]\n")
    console.print("[yellow]Use: celery -A app.celery_app worker -B --loglevel=info[/yellow]\n")


@cli.command()
def monitor():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å Flower –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥."""
    console.print("\n[bold blue]üå∏ Starting Flower monitor...[/bold blue]\n")
    console.print("[yellow]Use: celery -A app.celery_app flower[/yellow]")
    console.print("[yellow]Open: http://localhost:5555[/yellow]\n")


# ===== UTILITY FUNCTIONS =====

def display_scrape_stats(stats: dict):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    table = Table(show_header=True, header_style="bold cyan")
    table.add_column("Metric", style="yellow")
    table.add_column("Value", style="green")
    
    table.add_row("Total Articles", str(stats.get('total_articles', 0)))
    table.add_row("RSS Articles", str(stats.get('rss_articles', 0)))
    table.add_row("API Articles", str(stats.get('api_articles', 0)))
    table.add_row("Sent to Backend", str(stats.get('sent_to_backend', 0)))
    table.add_row("Errors", str(stats.get('errors', 0)))
    
    console.print(table)
    console.print()


def display_articles_table(articles: list, max_articles: int = 10):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–µ–π."""
    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("Title", style="cyan", no_wrap=False, width=50)
    table.add_column("Source", style="green", width=15)
    table.add_column("Published", style="yellow", width=20)
    
    for article in articles[:max_articles]:
        # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        title = article.title[:47] + "..." if len(article.title) > 50 else article.title
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
        pub_date = article.published_at.strftime("%Y-%m-%d %H:%M") if article.published_at else "Unknown"
        
        table.add_row(title, article.source, pub_date)
    
    console.print(table)
    console.print()


# ===== MAIN =====

if __name__ == '__main__':
    cli()


# ===== USAGE EXAMPLES =====
"""
# ===== CLI Usage =====

# Scrape all sources
python -m app.main scrape-all

# Async scrape
python -m app.main scrape-all --async

# RSS only
python -m app.main scrape-rss

# News API only
python -m app.main scrape-api

# Specific source
python -m app.main scrape bbc
python -m app.main scrape techcrunch

# List sources
python -m app.main list-sources

# Show config
python -m app.main config-info

# Test scraper
python -m app.main test --source bbc

# Help
python -m app.main --help
python -m app.main scrape --help


# ===== Celery Usage =====

# Start worker (sync)
celery -A app.celery_app worker --loglevel=info

# Start worker + beat (periodic tasks)
celery -A app.celery_app worker -B --loglevel=info

# Start flower monitoring
celery -A app.celery_app flower

# Multiple queues
celery -A app.celery_app worker -Q scraping,processing --loglevel=info

# Concurrency
celery -A app.celery_app worker --concurrency=4
"""