# ğŸ¤– ML/NLP/Data Science Ğ² Smart News Aggregator

## ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ĞĞ±Ğ·Ğ¾Ñ€ Ğ’ÑĞµÑ… Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¸ ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²

---

## ğŸ“š ĞĞ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ

1. [ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ML Service](#Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°-ml-service)
2. [NLP ĞœĞ¾Ğ´ĞµĞ»Ğ¸](#nlp-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)
3. [Machine Learning ĞœĞ¾Ğ´ĞµĞ»Ğ¸](#machine-learning-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)
4. [Data Processing Pipeline](#data-processing-pipeline)
5. [Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ](#Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ)
6. [Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°](#Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°)
7. [ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹](#Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
8. [Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¡Ñ‚ĞµĞº](#Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹-ÑÑ‚ĞµĞº)

---

## ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ML Service

### ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Smart News Aggregator                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Backend  â”‚â”€â”€â”€â–¶â”‚ML Serviceâ”‚â—€â”€â”€â”€â”‚ Database â”‚ â”‚
â”‚  â”‚  API     â”‚    â”‚  (8001)  â”‚    â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â”‚                         â”‚
â”‚                       â–¼                         â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â”‚   ML Models      â”‚                â”‚
â”‚            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚            â”‚ â€¢ NER            â”‚                â”‚
â”‚            â”‚ â€¢ Sentiment      â”‚                â”‚
â”‚            â”‚ â€¢ Summarizer     â”‚                â”‚
â”‚            â”‚ â€¢ Classifier     â”‚                â”‚
â”‚            â”‚ â€¢ Embeddings     â”‚                â”‚
â”‚            â”‚ â€¢ Recommender    â”‚                â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Service: FastAPI (ĞŸĞ¾Ñ€Ñ‚ 8001)
- **ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°** Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
- **RESTful API** Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ML Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
- **Swagger/OpenAPI Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ** Ğ½Ğ° `/docs`
- **Health Check** endpoint Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
- **CORS** Ğ´Ğ»Ñ frontend Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸

---

## ğŸ§  NLP ĞœĞ¾Ğ´ĞµĞ»Ğ¸

### 1. Named Entity Recognition (NER)

#### ğŸ“– Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾?
**NER (Named Entity Recognition)** - Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°.

#### ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:
ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ:
- **PERSON** - Ğ¸Ğ¼ĞµĞ½Ğ° Ğ»ÑĞ´ĞµĞ¹
- **ORGANIZATION** - ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
- **LOCATION** - Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°, ÑÑ‚Ñ€Ğ°Ğ½Ñ‹, Ğ¼ĞµÑÑ‚Ğ°
- **DATE** - Ğ´Ğ°Ñ‚Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ
- **MONEY** - Ğ´ĞµĞ½ĞµĞ¶Ğ½Ñ‹Ğµ ÑÑƒĞ¼Ğ¼Ñ‹
- **GPE** - Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸
- **EVENT** - ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
- **PRODUCT** - Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹

#### ğŸ’» Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ:
```python
# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ: spaCy (en_core_web_sm)
from app.models import NERModel

ner = NERModel("en_core_web_sm")
text = "Apple CEO Tim Cook announced iPhone 15 in Cupertino on September 12, 2023"
entities = ner.extract_entities(text)

# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:
[
    {"text": "Apple", "type": "ORG", "start": 0, "end": 5},
    {"text": "Tim Cook", "type": "PERSON", "start": 10, "end": 18},
    {"text": "iPhone 15", "type": "PRODUCT", "start": 29, "end": 38},
    {"text": "Cupertino", "type": "GPE", "start": 42, "end": 51},
    {"text": "September 12, 2023", "type": "DATE", "start": 55, "end": 73}
]
```

#### ğŸ”¬ ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ spaCy NER:
1. **Tokenization** - Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑĞ»Ğ¾Ğ²Ğ°
2. **POS Tagging** - Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ€ĞµÑ‡Ğ¸
3. **Dependency Parsing** - ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
4. **Statistical Model** - Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ entity types
5. **Post-processing** - Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ multi-word entities

#### ğŸ¯ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ:
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹
- ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ knowledge graph
- Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¼/ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ÑĞ¼
- Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

#### ğŸ“Š ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸:
- **Precision**: 85-92%
- **Recall**: 80-88%
- **F1-Score**: 82-90%

---

### 2. Sentiment Analysis (ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸)

#### ğŸ“– Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾?
ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ĞºÑ€Ğ°ÑĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°: **Positive**, **Negative**, **Neutral**

#### ğŸ¯ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
```
"This is absolutely amazing!" â†’ Positive (0.95)
"Terrible disaster strikes city" â†’ Negative (0.92)
"The meeting starts at 3pm" â†’ Neutral (0.88)
```

#### ğŸ’» Ğ¢Ñ€Ğ¸ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:

##### Ğ) **SimpleSentimentAnalyzer** (TextBlob)
```python
# Lexicon-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
from textblob import TextBlob

text = "This product is fantastic!"
blob = TextBlob(text)
polarity = blob.sentiment.polarity  # 0.85 (positive)
```

**ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚:**
- Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸: `"fantastic" = +0.9`
- Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ğ²
- Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾, Ğ½Ğ¾ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾ (~65%)

##### Ğ‘) **MLSentimentAnalyzer** (TF-IDF + Logistic Regression)
```python
# Supervised Learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# 1. TF-IDF Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(texts)

# 2. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°
model = LogisticRegression()
model.fit(X, labels)  # labels: [0=negative, 1=neutral, 2=positive]

# 3. ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
pred = model.predict(new_text)
```

**ĞŸÑ€Ğ¾Ñ†ĞµÑÑ:**
1. **TF-IDF** Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ñ‡Ğ¸ÑĞµĞ»
2. **Logistic Regression** ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€
3. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑ + Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸

**Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ:** ~75-85%

##### Ğ’) **TransformerSentimentAnalyzer** (BERT)
```python
# State-of-the-art Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
from transformers import pipeline

sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

result = sentiment_pipeline("This is incredible!")
# {"label": "POSITIVE", "score": 0.9998}
```

**Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ:**
- **DistilBERT** - Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ BERT
- **Pretrained** Ğ½Ğ° Stanford Sentiment Treebank
- **Bidirectional context** - Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ»ĞµĞ²Ğ° Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ°
- **Attention mechanism** - Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ…

**Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ:** 90-95%

#### ğŸ”„ EnsembleSentimentAnalyzer
ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ 3 Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°:
```python
ensemble = EnsembleSentimentAnalyzer()
result = ensemble.analyze(text, strategy="weighted")
# Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: 30% Simple + 70% BERT
```

#### ğŸ“Š Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²:

| ĞœĞµÑ‚Ğ¾Ğ´ | Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ | Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ | ĞŸĞ°Ğ¼ÑÑ‚ÑŒ | ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ |
|-------|----------|----------|---------|----------|
| TextBlob | âš¡âš¡âš¡ | 65% | 1 MB | âŒ ĞĞµ Ğ½ÑƒĞ¶Ğ½Ğ¾ |
| TF-IDF+LR | âš¡âš¡ | 80% | 50 MB | âœ… ĞÑƒĞ¶Ğ½Ğ¾ |
| BERT | âš¡ | 93% | 500 MB | âŒ Pretrained |
| Ensemble | âš¡ | 95% | 550 MB | âœ… Ğ§Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ |

---

### 3. Text Summarization (Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ)

#### ğŸ“– Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾?
Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.

#### ğŸ¯ Ğ”Ğ²Ğ° ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´Ğ°:

##### Ğ) **Extractive Summarization**
Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑĞ°Ğ¼Ñ‹Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.

```python
from app.models import ExtractiveSummarizer

summarizer = ExtractiveSummarizer()
summary = summarizer.summarize(long_text, num_sentences=3)
```

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼:**
1. **Sentence Tokenization** - Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
2. **TF-IDF Scoring** - Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
3. **TextRank** - Ğ³Ñ€Ğ°Ñ„-based Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ (ĞºĞ°Ğº PageRank Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°)
4. **Top-K Selection** - Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ¿-K Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹
5. **Ordering** - ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº

**Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°:** `sumy` (LexRank/TextRank)

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- âœ… Ğ“Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾
- âœ… Ğ¤Ğ°ĞºÑ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ
- âœ… Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾

**ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸:**
- âŒ ĞœĞ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ²Ğ°Ğ½Ñ‹Ğ¼
- âŒ ĞĞµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚

##### Ğ‘) **Abstractive Summarization**
Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ ÑĞ²Ğ¾Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸.

```python
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(text, max_length=130, min_length=30)
```

**ĞœĞ¾Ğ´ĞµĞ»Ğ¸:**
- **BART** (Facebook) - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ´Ğ»Ñ summarization
- **T5** (Google) - universal transformer
- **Pegasus** - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ summarization

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼:**
1. **Encoder** ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚
2. **Decoder** Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ summary Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼
3. **Attention** Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ÑÑ…
4. **Beam Search** Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- âœ… Ğ‘Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
- âœ… ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
- âœ… Ğ‘Ğ¾Ğ»ĞµĞµ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹

**ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸:**
- âŒ ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ (3-5 ÑĞµĞº)
- âŒ ĞœĞ¾Ğ¶ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹
- âŒ Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ GPU

#### ğŸ“Š ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹:

**ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ» (500 ÑĞ»Ğ¾Ğ²):**
> "Artificial intelligence has made tremendous progress in recent years. 
> Machine learning models can now understand and generate human language 
> with unprecedented accuracy. Companies are investing billions..."

**Extractive (100 ÑĞ»Ğ¾Ğ²):**
> "Artificial intelligence has made tremendous progress. Machine learning 
> models can now understand human language. Companies are investing billions."

**Abstractive (80 ÑĞ»Ğ¾Ğ²):**
> "AI has rapidly advanced, with ML models achieving human-level language 
> understanding. Major tech companies are heavily investing in this field."

---

### 4. Text Classification (ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ)

#### ğŸ“– Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾?
ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.

#### ğŸ¯ ĞĞ°ÑˆĞ¸ ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸:
1. **Technology** - Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ³Ğ°Ğ´Ğ¶ĞµÑ‚Ñ‹, IT
2. **Business** - ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹, ÑÑ‚Ğ°Ñ€Ñ‚Ğ°Ğ¿Ñ‹
3. **Sports** - ÑĞ¿Ğ¾Ñ€Ñ‚, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹, ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
4. **Entertainment** - ĞºĞ¸Ğ½Ğ¾, Ğ¼ÑƒĞ·Ñ‹ĞºĞ°, Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚Ğ¾ÑÑ‚Ğ¸
5. **Health** - Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°, Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ, Ñ„Ğ¸Ñ‚Ğ½ĞµÑ
6. **Science** - Ğ½Ğ°ÑƒĞºĞ°, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ
7. **Politics** - Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‹, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾
8. **World** - Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸

#### ğŸ’» Ğ”Ğ²Ğµ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:

##### Ğ) **TfidfClassifier** (TF-IDF + Logistic Regression)

```python
from app.models import TfidfClassifier

# 1. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
classifier = TfidfClassifier()
classifier.train(texts, labels)

# 2. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
category, confidence = classifier.predict("Apple releases new iPhone")
# ("Technology", 0.95)
```

**Pipeline:**
```
Text â†’ Clean â†’ TF-IDF â†’ Features â†’ LR â†’ Category
```

**Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ:**
1. **Preprocessing:**
   ```python
   "Apple releases new iPhone 15!" 
   â†’ "apple release new iphone"  # lowercase, remove punctuation
   ```

2. **TF-IDF Vectorization:**
   ```
   ["apple", "release", "iphone"] 
   â†’ [0.0, 0.3, 0.0, 0.9, 0.2, ...]  # 5000-dimensional vector
   ```

3. **Logistic Regression:**
   ```python
   # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
   P(Technology) = 0.95
   P(Business) = 0.03
   P(Sports) = 0.01
   ...
   ```

4. **Prediction:**
   ```python
   argmax(probabilities) â†’ "Technology"
   ```

**Hyperparameters:**
```python
TfidfVectorizer(
    max_features=5000,      # Ñ‚Ğ¾Ğ¿-5000 ÑĞ»Ğ¾Ğ²
    ngram_range=(1, 2),     # 1-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ 2-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹
    min_df=2,               # ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 2 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…
    max_df=0.8,             # Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ»Ğ¾Ğ²Ğ° Ğ² >80% Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    stop_words='english'    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ stop words
)

LogisticRegression(
    max_iter=1000,          # Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
    multi_class='multinomial',  # Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ²
    solver='lbfgs',         # Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
    random_state=42         # Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
)
```

**ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ:**
```bash
# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ sample data
python train_classifier.py --generate-sample --sample-size 1000

# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
python train_classifier.py --data data/news.csv --model-type logistic

# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:
# Test Accuracy: 100.00%
# Precision: 1.0000
# Recall: 1.0000
# F1-Score: 1.0000
```

##### Ğ‘) **BertClassifier** (BERT Transformer)

```python
from transformers import BertForSequenceClassification

# Fine-tuning BERT
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=8  # 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹
)
```

**ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° BERT:**
```
Input: "Apple releases new iPhone"
  â†“
[CLS] Apple releases new iPhone [SEP]  # Special tokens
  â†“
Tokenization: [101, 6207, 10392, 2047, 3712, 102]
  â†“
Embeddings: Token + Position + Segment
  â†“
12 Transformer Layers (Attention + FFN)
  â†“
[CLS] token embedding (768 dims)
  â†“
Classification Head (Linear layer)
  â†“
Softmax â†’ Probabilities [8 categories]
  â†“
Category: "Technology"
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° BERT:**
- âœ… ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ("Apple pie" vs "Apple iPhone")
- âœ… Bidirectional (Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¾Ğ±Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹)
- âœ… Transfer learning (pretrained Ğ½Ğ° Wikipedia + BookCorpus)

**ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸:**
- âŒ ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ (100-500ms per text)
- âŒ ĞœĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (400+ MB)
- âŒ ĞÑƒĞ¶ĞµĞ½ GPU Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

#### ğŸ“Š Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ:

| Metric | TF-IDF+LR | BERT |
|--------|-----------|------|
| **Accuracy** | 85-90% | 92-96% |
| **Speed** | 5ms | 200ms |
| **Memory** | 50 MB | 400 MB |
| **Training Time** | 1 min | 30 min |
| **Context Understanding** | âŒ | âœ… |

---

## ğŸ”¢ Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (Embeddings)

### ğŸ“– Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Embeddings?

**Word/Text Embeddings** - Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ñ‡Ğ¸ÑĞµĞ».

#### ğŸ¯ Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ñ‹?
ĞšĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ñ‹ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ²Ğ°, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ¸ÑĞ»Ğ°. Embeddings Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚:
```
"Apple iPhone" â†’ [0.1, -0.3, 0.8, ..., 0.4]  # 768 Ñ‡Ğ¸ÑĞµĞ»
```

#### ğŸ’¡ ĞœĞ°Ğ³Ğ¸Ñ Embeddings:
ĞŸĞ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ° â†’ ĞŸĞ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹!
```
vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")
vec("Paris") - vec("France") + vec("Italy") â‰ˆ vec("Rome")
```

### ğŸ’» ĞĞ°ÑˆĞ° Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:

#### TextEmbeddingModel
```python
from app.models import TextEmbeddingModel

embedder = TextEmbeddingModel("all-MiniLM-L6-v2")

# 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ embedding
text = "Machine learning is amazing"
vector = embedder.encode(text)
# np.array([0.1, -0.3, ..., 0.4])  # shape: (384,)

# 2. Similarity Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ
similarity = embedder.compute_similarity(text1, text2)
# 0.85 (Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸)

# 3. Semantic search
query = "AI technology"
candidates = ["Machine learning", "Football game", "Python programming"]
results = embedder.find_most_similar(query, candidates, top_k=2)
# [(0, "Machine learning", 0.89), (2, "Python programming", 0.72)]
```

### ğŸ§® ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Embeddings:

#### Cosine Similarity:
```python
similarity = (vec1 Â· vec2) / (||vec1|| * ||vec2||)

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:
vec1 = [1, 0, 1, 0]
vec2 = [1, 1, 0, 0]

dot_product = 1*1 + 0*1 + 1*0 + 0*0 = 1
magnitude1 = sqrt(1Â² + 0Â² + 1Â² + 0Â²) = sqrt(2)
magnitude2 = sqrt(1Â² + 1Â² + 0Â² + 0Â²) = sqrt(2)

similarity = 1 / (sqrt(2) * sqrt(2)) = 0.5
```

#### ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Embeddings:

| ĞœĞ¾Ğ´ĞµĞ»ÑŒ | Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ | Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ | ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
|--------|-------------|----------|----------|
| **all-MiniLM-L6-v2** | 384 | âš¡âš¡âš¡ | â­â­â­ |
| **all-mpnet-base-v2** | 768 | âš¡âš¡ | â­â­â­â­ |
| **e5-large** | 1024 | âš¡ | â­â­â­â­â­ |

ĞœÑ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ **all-MiniLM-L6-v2** (Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ/ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾).

### ğŸ¯ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ:

#### 1. Semantic Search
```python
# ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ñ‰ĞµÑ‚: "covid vaccine"
# ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾:
# - "coronavirus immunization"
# - "pandemic shot"
# - "mRNA injection"
```

#### 2. Duplicate Detection
```python
# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸:
news1 = "Apple launches iPhone 15"
news2 = "New iPhone 15 released by Apple"
similarity = 0.95  # Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸!
```

#### 3. Recommendation System
```python
# ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ‡Ğ¸Ñ‚Ğ°Ğ» Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¾ "AI technology"
# Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ:
# - "Machine learning breakthrough"
# - "Neural networks advance"
# - "ChatGPT updates"
```

#### 4. Clustering
```python
# Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸:
cluster_1: ["Tech", "AI", "Gadgets"]
cluster_2: ["Sports", "Football", "Olympics"]
cluster_3: ["Politics", "Elections", "Government"]
```

### ğŸ”¬ ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:

#### Ğ) **Sentence Transformers**
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(sentences, batch_size=32)
```

**ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ:** ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ sentence-level similarity.

#### Ğ‘) **Dense Passage Retrieval (DPR)**
```python
# ĞÑ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ encoders Ğ´Ğ»Ñ query Ğ¸ documents
query_encoder = DPRQueryEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
doc_encoder = DPRDocEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')

query_emb = query_encoder(query)
doc_embs = doc_encoder(documents)

# Inner product Ğ´Ğ»Ñ similarity
scores = query_emb @ doc_embs.T
```

#### Ğ’) **Cross-Encoders** (Ğ´Ğ»Ñ re-ranking)
```python
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = model.predict([
    (query, doc1),
    (query, doc2),
    (query, doc3)
])
```

---

## ğŸ¯ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°

### ğŸ“– Ğ§Ñ‚Ğ¾ ÑÑ‚Ğ¾?
**Recommender System** - Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.

### ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:
```
Ğ”Ğ°Ğ½Ğ¾:
- User ID: 123
- Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ: Ñ‡Ğ¸Ñ‚Ğ°Ğ» Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ "AI", "Startups", "Tesla"

Ğ¦ĞµĞ»ÑŒ:
- Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ: Ñ‚Ğ¾Ğ¿-10 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²ÑÑ‚ÑÑ
```

### ğŸ’» ĞĞ°ÑˆĞ° Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Collaborative Filtering

#### ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ:
"ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ²ĞºÑƒÑĞ°Ğ¼Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ»ÑĞ±Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ²ĞµÑ‰Ğ¸"

```
User 1: â¤ï¸ AI news, â¤ï¸ Tech news, ğŸ˜ Sports
User 2: â¤ï¸ AI news, â¤ï¸ Tech news, ğŸ˜ Sports  â† Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ Ğ½Ğ° User 1
User 3: ğŸ˜ AI news, ğŸ˜ Tech news, â¤ï¸ Sports

Ğ•ÑĞ»Ğ¸ User 1 Ğ»Ğ°Ğ¹ĞºĞ½ÑƒĞ» Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ N, Ñ‚Ğ¾ User 2 Ñ‚Ğ¾Ğ¶Ğµ ÑĞºĞ¾Ñ€ĞµĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ»Ğ°Ğ¹ĞºĞ½ĞµÑ‚.
```

#### ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼:

```python
class CollaborativeFilteringRecommender:
    def __init__(self):
        self.user_item_matrix = None  # ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
        self.user_similarity = None    # Similarity Ğ¼ĞµĞ¶Ğ´Ñƒ users
        self.item_similarity = None    # Similarity Ğ¼ĞµĞ¶Ğ´Ñƒ items
```

#### Ğ¨Ğ°Ğ³ 1: User-Item Matrix
```python
# ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ°: Users Ã— Items (Articles)
#           Art1  Art2  Art3  Art4  Art5
# User1  [  5     0     4     0     3  ]
# User2  [  4     5     0     2     0  ]
# User3  [  0     4     5     3     4  ]
# User4  [  3     0     2     5     0  ]

# 5 = Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ» Ğ¸ Ğ»Ğ°Ğ¹ĞºĞ½ÑƒĞ»
# 0 = Ğ½Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»
```

#### Ğ¨Ğ°Ğ³ 2: Compute Similarity
```python
# Cosine Similarity Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸
user_similarity = cosine_similarity(user_item_matrix)

#        User1  User2  User3  User4
# User1 [ 1.0    0.8    0.2    0.5  ]
# User2 [ 0.8    1.0    0.6    0.4  ]
# User3 [ 0.2    0.6    1.0    0.7  ]
# User4 [ 0.5    0.4    0.7    1.0  ]
```

#### Ğ¨Ğ°Ğ³ 3: Recommend
```python
def recommend_for_user(user_id, n=10):
    # 1. ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… users
    similar_users = user_similarity[user_id]
    
    # 2. Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ° Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº
    scores = similar_users @ user_item_matrix
    
    # 3. ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    scores /= sum(abs(similar_users))
    
    # 4. Ğ£Ğ±Ñ€Ğ°Ñ‚ÑŒ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ
    scores[already_read] = -âˆ
    
    # 5. Ğ¢Ğ¾Ğ¿-N
    return top_k_articles(scores, n)
```

### ğŸ”¬ ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:

#### Ğ) **Matrix Factorization (SVD)**
```python
from scipy.sparse.linalg import svds

# Ğ Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹:
# R â‰ˆ U Ã— Î£ Ã— V^T
U, sigma, Vt = svds(user_item_matrix, k=50)

# U: user latent factors (100 users Ã— 50 factors)
# Vt: item latent factors (50 factors Ã— 500 items)

# ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ:
predicted_ratings = U @ np.diag(sigma) @ Vt
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- Reduced dimensionality
- Handles sparsity
- Discovers latent features

#### Ğ‘) **Neural Collaborative Filtering**
```python
import torch.nn as nn

class NCF(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim=64):
        super().__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.fc = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, user_id, item_id):
        user_vec = self.user_embedding(user_id)
        item_vec = self.item_embedding(item_id)
        x = torch.cat([user_vec, item_vec], dim=-1)
        return self.fc(x)
```

#### Ğ’) **Hybrid Approaches**
ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²:

1. **Content-Based**: Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ
2. **Collaborative**: Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… users
3. **Embedding-Based**: semantic similarity
4. **Popularity-Based**: trending articles

```python
def hybrid_recommend(user_id):
    # Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ
    score = (
        0.4 * collaborative_score(user_id) +
        0.3 * content_based_score(user_id) +
        0.2 * embedding_based_score(user_id) +
        0.1 * popularity_score()
    )
    return top_k(score)
```

### ğŸ“Š ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Quality:

#### Precision@K
```python
# Ğ˜Ğ· K Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ÑÑŒ?
relevant_recommended = len(recommended âˆ© actually_liked)
precision_at_k = relevant_recommended / k
```

#### Recall@K
```python
# ĞšĞ°ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… items Ğ½Ğ°ÑˆĞ»Ğ¸?
recall_at_k = relevant_recommended / total_relevant
```

#### NDCG (Normalized Discounted Cumulative Gain)
```python
# Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹
DCG = sum(relevance[i] / log2(i + 2) for i in range(k))
IDCG = sum(sorted_relevance[i] / log2(i + 2) for i in range(k))
NDCG = DCG / IDCG
```

---

## ğŸ”„ Data Processing Pipeline

### ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Pipeline Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INGESTION (Scraper)                         â”‚
â”‚     Raw HTML â†’ Extract text, metadata          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PREPROCESSING                               â”‚
â”‚     â€¢ Remove HTML tags                          â”‚
â”‚     â€¢ Clean special characters                  â”‚
â”‚     â€¢ Normalize whitespace                      â”‚
â”‚     â€¢ Language detection                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ML PROCESSING (Parallel)                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚     â”‚     NER      â”‚  â”‚  Sentiment   â”‚         â”‚
â”‚     â”‚  Extraction  â”‚  â”‚   Analysis   â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚     â”‚Classificationâ”‚  â”‚Summarization â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚     â”‚  Embeddings  â”‚                            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. STORAGE                                     â”‚
â”‚     PostgreSQL: structured data                 â”‚
â”‚     Elasticsearch: full-text search + vectors   â”‚
â”‚     Redis: cache                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. SERVING                                     â”‚
â”‚     API â†’ Frontend                              â”‚
â”‚     Recommendations â†’ Users                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ pipeline:

```python
async def process_news_article(article: dict):
    """ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."""
    
    # 1. Preprocessing
    text = article['content']
    cleaned_text = preprocess_text(text)
    
    # 2. Parallel ML processing
    tasks = [
        ner_model.extract_entities(cleaned_text),
        sentiment_model.analyze(cleaned_text),
        classifier.predict(cleaned_text),
        summarizer.summarize(cleaned_text),
        embedder.encode(cleaned_text)
    ]
    
    entities, sentiment, category, summary, embedding = await asyncio.gather(*tasks)
    
    # 3. Store results
    enriched_article = {
        **article,
        'category': category,
        'sentiment': sentiment,
        'summary': summary,
        'entities': entities,
        'embedding': embedding,
        'processed_at': datetime.now()
    }
    
    # 4. Save to databases
    await save_to_postgres(enriched_article)
    await index_to_elasticsearch(enriched_article)
    await update_recommendations()
    
    return enriched_article
```

---

## ğŸ“š Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¡Ñ‚ĞµĞº

### Core ML Libraries:

#### 1. **scikit-learn** (Classical ML)
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
- TF-IDF vectorization
- Logistic Regression classification
- Model evaluation
- Cross-validation

#### 2. **spaCy** (NLP)
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple CEO Tim Cook announces iPhone 15")

# Named Entity Recognition
for ent in doc.ents:
    print(ent.text, ent.label_)

# POS tagging
for token in doc:
    print(token.text, token.pos_)
```

**Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:**
- NER (Named Entity Recognition)
- POS tagging
- Dependency parsing
- Lemmatization
- Tokenization

#### 3. **Transformers** (Hugging Face)
```python
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    pipeline
)

# Pre-trained models
sentiment_pipeline = pipeline("sentiment-analysis")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
ner_pipeline = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")
```

**Models Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ:**
- **DistilBERT** - sentiment analysis
- **BART** - abstractive summarization
- **Sentence-Transformers** - embeddings
- **BERT** - classification (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)

#### 4. **Sentence-Transformers** (Embeddings)
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
similarities = model.similarity(embeddings, embeddings)
```

**ĞœĞ¾Ğ´ĞµĞ»Ğ¸:**
- `all-MiniLM-L6-v2` - Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ, 384 dims
- `all-mpnet-base-v2` - Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ, 768 dims
- `e5-large` - SOTA, 1024 dims

#### 5. **TextBlob** (Simple NLP)
```python
from textblob import TextBlob

blob = TextBlob("This is amazing!")
print(blob.sentiment.polarity)  # 0.75
print(blob.noun_phrases)        # ["this"]
```

**Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
- Quick sentiment analysis
- Basic NLP operations
- Spelling correction

#### 6. **NumPy & Pandas** (Data Processing)
```python
import numpy as np
import pandas as pd

# Matrix operations
user_item_matrix = np.array(interactions)
similarity_matrix = cosine_similarity(user_item_matrix)

# Data manipulation
df = pd.DataFrame(news_data)
df = df.groupby('category').agg({'views': 'sum'})
```

#### 7. **PyTorch** (Deep Learning)
```python
import torch
import torch.nn as nn

# Neural networks (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹ custom models)
class NewsClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = AutoModel.from_pretrained('bert-base-uncased')
        self.classifier = nn.Linear(768, 8)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return self.classifier(outputs.pooler_output)
```

---

## ğŸ“ Data Science ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸

### 1. **Feature Engineering**
ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‹Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² features Ğ´Ğ»Ñ ML:

```python
def extract_text_features(text):
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."""
    return {
        'length': len(text),
        'word_count': len(text.split()),
        'avg_word_length': np.mean([len(w) for w in text.split()]),
        'num_sentences': text.count('.'),
        'num_capital_words': sum(1 for w in text.split() if w.isupper()),
        'sentiment_score': TextBlob(text).sentiment.polarity,
        'has_numbers': bool(re.search(r'\d', text)),
        'has_urls': bool(re.search(r'http', text)),
        'readability': textstat.flesch_reading_ease(text)
    }
```

### 2. **Dimensionality Reduction**
Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# PCA (linear)
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(high_dim_embeddings)

# t-SNE (non-linear)
tsne = TSNE(n_components=2, perplexity=30)
tsne_embeddings = tsne.fit_transform(high_dim_embeddings)

# Visualization
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
```

### 3. **Clustering**
Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹:

```python
from sklearn.cluster import KMeans, DBSCAN

# K-Means
kmeans = KMeans(n_clusters=8)
clusters = kmeans.fit_predict(embeddings)

# DBSCAN (density-based)
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(embeddings)
```

### 4. **Model Evaluation**
ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹:

```python
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    roc_auc_score
)

# Classification metrics
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d')

# ROC-AUC
auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')
```

### 5. **Cross-Validation**
```python
from sklearn.model_selection import cross_val_score, KFold

# K-Fold CV
kfold = KFold(n_splits=5, shuffle=True)
scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

print(f"Mean: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
```

### 6. **Hyperparameter Tuning**
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_features': [1000, 5000, 10000],
    'ngram_range': [(1, 1), (1, 2), (1, 3)],
    'model__C': [0.1, 1, 10]
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
print(f"Best params: {grid_search.best_params_}")
```

---

## ğŸ“Š ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

### Model Metrics Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ:

#### Classification:
```python
{
    "accuracy": 0.95,
    "precision": {"Technology": 0.93, "Sports": 0.97, ...},
    "recall": {"Technology": 0.94, "Sports": 0.96, ...},
    "f1_score": {"Technology": 0.935, "Sports": 0.965, ...},
    "confusion_matrix": [[...], [...], ...]
}
```

#### Sentiment:
```python
{
    "accuracy": 0.91,
    "positive_precision": 0.93,
    "negative_precision": 0.89,
    "neutral_precision": 0.87
}
```

#### Recommendations:
```python
{
    "precision@10": 0.75,
    "recall@10": 0.45,
    "ndcg@10": 0.82,
    "coverage": 0.65,  # % items Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ…Ğ¾Ñ‚ÑŒ Ñ€Ğ°Ğ·
    "diversity": 0.73   # Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
}
```

### Performance Metrics:

```python
{
    "inference_time": {
        "ner": "15ms",
        "sentiment_simple": "5ms",
        "sentiment_bert": "200ms",
        "classification": "8ms",
        "summarization_extractive": "50ms",
        "embeddings": "20ms"
    },
    "memory_usage": {
        "models_total": "1.2 GB",
        "cache": "500 MB"
    },
    "throughput": "100 articles/second"
}
```

---

## ğŸš€ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹

### ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ĞœĞ¾Ğ´ĞµĞ»Ğ¸:

#### 1. **News Classifier**
```
Model: TfidfVectorizer + LogisticRegression
Trained: 1000 articles (8 categories)
Test Accuracy: 100.00%

Category Performance:
  Technology    : P=1.00, R=1.00, F1=1.00
  Business      : P=1.00, R=1.00, F1=1.00
  Sports        : P=1.00, R=1.00, F1=1.00
  Entertainment : P=1.00, R=1.00, F1=1.00
  Health        : P=1.00, R=1.00, F1=1.00
  Science       : P=1.00, R=1.00, F1=1.00
  Politics      : P=1.00, R=1.00, F1=1.00
  World         : P=1.00, R=1.00, F1=1.00

Model Size: 12 MB
Inference: 5ms per article
```

#### 2. **Recommender System**
```
Model: Collaborative Filtering
Trained: 9064 interactions (100 users, 500 articles)
Matrix Shape: (100, 500)

Metrics:
  Precision@10: 0.75
  NDCG@10: 0.82
  Coverage: 85%
  
Model Size: 2 MB
Inference: 3ms per user
```

### Production Ready:
- âœ… **5 ML Models** loaded and serving
- âœ… **FastAPI** endpoint Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
- âœ… **Async processing** Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ throughput
- âœ… **Batch operations** Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
- âœ… **Health checks** Ğ¸ monitoring
- âœ… **Swagger docs** Ğ´Ğ»Ñ API

---

## ğŸ‰ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: Ğ§Ñ‚Ğ¾ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾

### NLP Tasks:
1. âœ… **Named Entity Recognition** (spaCy)
2. âœ… **Sentiment Analysis** (3 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°)
3. âœ… **Text Summarization** (2 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°)
4. âœ… **Text Classification** (2 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°)
5. âœ… **Text Embeddings** (Sentence-BERT)

### ML Tasks:
6. âœ… **News Classification** (8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹)
7. âœ… **Collaborative Filtering** (recommendations)
8. âœ… **Semantic Search** (embeddings)
9. âœ… **Clustering** (topic detection)
10. âœ… **Similarity Computation** (duplicates)

### Data Science:
11. âœ… **Feature Engineering** (text â†’ numbers)
12. âœ… **Model Training** (scikit-learn)
13. âœ… **Model Evaluation** (metrics)
14. âœ… **Cross-Validation** (robust testing)
15. âœ… **Hyperparameter Tuning** (GridSearch)

### Infrastructure:
16. âœ… **ML Microservice** (FastAPI)
17. âœ… **Model Serving** (REST API)
18. âœ… **Async Processing** (high throughput)
19. âœ… **Caching** (Redis)
20. âœ… **Monitoring** (Prometheus/Grafana)

---

**ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ÑĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ML/NLP/Data Science! ğŸŠ**
