# ğŸ•·ï¸ ĞšĞ°Ğº Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¡Ğ±Ğ¾Ñ€ ĞĞ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ - Smart News Aggregator

## ğŸ“Œ ĞÑ‚ĞºÑƒĞ´Ğ° Ğ‘ĞµÑ€ÑƒÑ‚ÑÑ ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸?

Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ **2 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°** Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹:

### 1. **RSS Feeds** (ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº) ğŸ“°
- Ğ‘ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹
- ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ»ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
- ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ **15 Ğ¼Ğ¸Ğ½ÑƒÑ‚**
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ XML

### 2. **News API** (Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹) ğŸŒ
- ĞĞ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚ 80,000+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²
- Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ API key (Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ 100 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²/Ğ´ĞµĞ½ÑŒ)
- ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ **30 Ğ¼Ğ¸Ğ½ÑƒÑ‚**
- REST API Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ

---

## ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¡Ğ±Ğ¾Ñ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SCRAPER SERVICE                      â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Celery    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Redis     â”‚            â”‚
â”‚  â”‚   Beat      â”‚ Tasks   â”‚   Broker    â”‚            â”‚
â”‚  â”‚ (Scheduler) â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚                    â”‚
â”‚                                  â”‚                    â”‚
â”‚                                  â–¼                    â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                         â”‚ Celery Workerâ”‚             â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                  â”‚                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚                        â”‚          â”‚        â”‚
â”‚         â–¼                        â–¼          â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ RSS Scraperâ”‚         â”‚ API Scraperâ”‚  â”‚ Others â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                        â”‚                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                      â”‚                                â”‚
â”‚                      â–¼                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚ Deduplicationâ”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                      â”‚                                â”‚
â”‚                      â–¼                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚ Send to API  â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    BACKEND API          â”‚
         â”‚  POST /api/v1/news/batchâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    PostgreSQL           â”‚
         â”‚  (News Database)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ ĞšĞ°Ğº Ğ­Ñ‚Ğ¾ Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚

### 1. **Scheduler (Celery Beat)**
- Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ
- ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ **15 Ğ¼Ğ¸Ğ½ÑƒÑ‚** Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ RSS scraping
- ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ **30 Ğ¼Ğ¸Ğ½ÑƒÑ‚** Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ scraping (RSS + API)
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ² `scraper_service/app/celery_app.py`

### 2. **Workers (Celery Workers)**
- ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Redis queue
- Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾
- ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ retry
- Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ÑĞµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸

### 3. **RSS Scraper**
**Ğ¤Ğ°Ğ¹Ğ»:** `scraper_service/app/scrapers/rss_scraper.py`

**Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚:**
```python
# 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ RSS feed
feed = feedparser.parse('http://feeds.bbci.co.uk/news/rss.xml')

# 2. ĞŸĞ°Ñ€ÑĞ¸Ñ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑÑ‚Ğ°Ñ‚ÑŒÑ
for entry in feed.entries:
    article = Article(
        title=entry.title,
        url=entry.link,
        content=entry.description,
        published_at=entry.published_parsed
    )

# 3. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ğ°Ñ‚ĞµĞ¹
return articles
```

**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ RSS:**
- BBC News
- CNN
- Reuters
- TechCrunch
- The Verge
- Wired
- (Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ² config.py)

### 4. **News API Scraper**
**Ğ¤Ğ°Ğ¹Ğ»:** `scraper_service/app/scrapers/api_scraper.py`

**Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚:**
```python
# 1. ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğº News API
client = NewsApiClient(api_key=settings.NEWS_API_KEY)

# 2. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ top headlines
response = client.get_top_headlines(
    sources='bbc-news,cnn,techcrunch',
    language='en',
    page_size=100
)

# 3. ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² Article objects
articles = [parse_article(item) for item in response['articles']]

# 4. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
return articles
```

### 5. **Deduplication (Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²)**
```python
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾ URL Ğ¸ content hash
seen_urls = set()
unique_articles = []

for article in articles:
    if article.url not in seen_urls:
        unique_articles.append(article)
        seen_urls.add(article.url)
```

### 6. **ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ² Backend**
```python
# ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° batch Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼
response = requests.post(
    'http://backend:8000/api/v1/news/batch',
    json={'articles': articles}
)
```

---

## ğŸš€ ĞšĞ°Ğº Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Scraper

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: Ğ§ĞµÑ€ĞµĞ· Docker Compose (Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)

```bash
cd /mnt/c/Projects/smart-news-aggregator

# 1. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ scraper worker
docker-compose up -d scraper_worker

# 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸
docker-compose logs -f scraper_worker

# 3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· Flower (web UI)
docker-compose up -d flower
open http://localhost:5555
```

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ—Ğ°Ğ¿ÑƒÑĞº

```bash
cd scraper_service

# 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
pip install -r requirements.txt

# 2. ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ .env
export CELERY_BROKER_URL="redis://localhost:6379/0"
export BACKEND_URL="http://localhost:8000"
export NEWS_API_KEY="your-api-key-here"

# 3. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ worker + beat
celery -A app.celery_app worker -B --loglevel=info
```

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 3: Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸

```python
# Ğ’ Python shell Ğ¸Ğ»Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğµ
from scraper_service.app.tasks.scraping_tasks import scrape_all_sources

# Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ ÑĞµĞ¹Ñ‡Ğ°Ñ
result = scrape_all_sources.delay()

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ
print(result.status)  # PENDING, STARTED, SUCCESS

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
stats = result.get(timeout=600)
print(stats)
# Output:
# {
#     'total_articles': 150,
#     'rss_articles': 100,
#     'api_articles': 50,
#     'sent_to_backend': 145,
#     'errors': 5
# }
```

---

## ğŸ”§ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²

### Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ RSS Feed

**Ğ¤Ğ°Ğ¹Ğ»:** `scraper_service/app/config.py`

```python
RSS_FEEDS = {
    'bbc': 'http://feeds.bbci.co.uk/news/rss.xml',
    'cnn': 'http://rss.cnn.com/rss/edition.rss',
    'techcrunch': 'https://techcrunch.com/feed/',
    
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:
    'your_source': 'https://your-site.com/rss.xml'
}
```

### Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ News API Source

```python
NEWS_API_SOURCES = [
    'bbc-news',
    'cnn',
    'techcrunch',
    'the-verge',
    
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹:
    'your-source-id'  # ID Ğ¸Ğ· newsapi.org/sources
]
```

### ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°

```python
# Ğ’ celery_app.py

app.conf.beat_schedule = {
    'scrape-all-sources': {
        'task': 'app.tasks.scraping_tasks.scrape_all_sources',
        'schedule': 30 * 60.0,  # Ğ˜Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» (Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…)
    },
}
```

---

## ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Scraper

### 1. Flower Web UI

```bash
# Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Flower
docker-compose up -d flower

# ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ
open http://localhost:5555
```

**Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Flower:**
- âœ… ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
- âœ… Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
- âœ… Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑƒÑĞ¿ĞµÑ…Ğ°/Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
- âœ… Ğ“Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
- âœ… Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ workers

### 2. Ğ›Ğ¾Ğ³Ğ¸

```bash
# ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ»Ğ¾Ğ³Ğ¾Ğ² scraper
docker-compose logs -f scraper_worker

# Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ
docker-compose logs scraper_worker | grep ERROR

# ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 100 ÑÑ‚Ñ€Ğ¾Ğº
docker-compose logs --tail=100 scraper_worker
```

### 3. Redis CLI

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡
docker-compose exec redis redis-cli

> KEYS celery*
> LLEN celery  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸
```

---

## ğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Scraper

### Ğ¢ĞµÑÑ‚ 1: Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ’Ñ€ÑƒÑ‡Ğ½ÑƒÑ

```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ test_scraper.py
cat > test_scraper.py << 'EOF'
from scraper_service.app.tasks.scraping_tasks import scrape_all_sources
import time

print("Starting scrape...")
task = scrape_all_sources.delay()
print(f"Task ID: {task.id}")

while task.status != 'SUCCESS':
    print(f"Status: {task.status}")
    if task.status == 'FAILURE':
        print(f"Error: {task.result}")
        break
    time.sleep(5)

if task.status == 'SUCCESS':
    stats = task.result
    print("\nâœ… Scrape Complete!")
    print(f"Total articles: {stats['total_articles']}")
    print(f"Sent to backend: {stats['sent_to_backend']}")
EOF

python test_scraper.py
```

### Ğ¢ĞµÑÑ‚ 2: ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ RSS Parser

```bash
cat > test_rss.py << 'EOF'
from scraper_service.app.scrapers.rss_scraper import RSSFeedScraper

# ĞŸĞ°Ñ€ÑĞ¸Ğ¼ BBC RSS
scraper = RSSFeedScraper(
    source_name='bbc',
    feed_url='http://feeds.bbci.co.uk/news/rss.xml'
)

articles = scraper.run()
print(f"Parsed {len(articles)} articles from BBC")

for article in articles[:5]:  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 5
    print(f"\n- {article.title}")
    print(f"  URL: {article.url}")
    print(f"  Published: {article.published_at}")
EOF

python test_rss.py
```

### Ğ¢ĞµÑÑ‚ 3: ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ News API

```bash
cat > test_newsapi.py << 'EOF'
from scraper_service.app.scrapers.api_scraper import NewsAPIScraper
import os

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ API key
os.environ['NEWS_API_KEY'] = 'your-api-key-here'

scraper = NewsAPIScraper(
    sources=['bbc-news', 'cnn']
)

articles = scraper.run()
print(f"Parsed {len(articles)} articles from News API")

for article in articles[:5]:
    print(f"\n- {article.title}")
    print(f"  Source: {article.source}")
EOF

python test_newsapi.py
```

---

## ğŸ”‘ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ News API Key

1. **ĞŸĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚:**
   https://newsapi.org

2. **Ğ—Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ:**
   - Email
   - Password
   - Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Free Ğ¿Ğ»Ğ°Ğ½

3. **ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ API Key:**
   - ĞŸĞ¾ÑĞ»Ğµ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ â†’ Dashboard
   - Ğ¡ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ API Key

4. **Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² .env:**
   ```bash
   # scraper_service/.env
   NEWS_API_KEY=your-api-key-here
   ```

**Ğ›Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹ Free Ğ¿Ğ»Ğ°Ğ½Ğ°:**
- 100 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²/Ğ´ĞµĞ½ÑŒ
- Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 30 Ğ´Ğ½ĞµĞ¹
- Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° ~15 Ğ¼Ğ¸Ğ½ÑƒÑ‚

---

## ğŸ“ˆ ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹

### ĞŸÑ€Ğ¸ Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾Ğ¼ Ğ—Ğ°Ğ¿ÑƒÑĞºĞµ:

**Ğ›Ğ¾Ğ³Ğ¸ scraper_worker:**
```
[2025-10-20 15:00:00] INFO: ============================================================
[2025-10-20 15:00:00] INFO: STARTING FULL SCRAPE
[2025-10-20 15:00:00] INFO: ============================================================
[2025-10-20 15:00:01] INFO: 1/2 Scraping RSS feeds...
[2025-10-20 15:00:05] INFO: Fetched 45 articles from bbc
[2025-10-20 15:00:08] INFO: Fetched 38 articles from cnn
[2025-10-20 15:00:12] INFO: âœ… RSS: 83 articles
[2025-10-20 15:00:13] INFO: 2/2 Scraping News API...
[2025-10-20 15:00:18] INFO: âœ… News API: 67 articles
[2025-10-20 15:00:19] INFO: Processing 150 articles...
[2025-10-20 15:00:20] INFO: After dedup: 145 unique articles
[2025-10-20 15:00:25] INFO: âœ… Sent 145 articles to backend
[2025-10-20 15:00:25] INFO: ============================================================
[2025-10-20 15:00:25] INFO: SCRAPE COMPLETE
[2025-10-20 15:00:25] INFO: Total: 150 | Sent: 145 | Errors: 0
[2025-10-20 15:00:25] INFO: ============================================================
```

### Ğ’ Ğ‘Ğ°Ğ·Ğµ Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…:

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹
docker-compose exec postgres psql -U postgres -d news_aggregator -c \
  "SELECT COUNT(*) FROM news;"

# ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
docker-compose exec postgres psql -U postgres -d news_aggregator -c \
  "SELECT title, source, published_at FROM news ORDER BY published_at DESC LIMIT 5;"
```

### Ğ§ĞµÑ€ĞµĞ· API:

```bash
# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
curl http://localhost:8000/api/v1/news/fresh | jq '.[].title'

# Output:
# "Breaking: Major Tech Announcement"
# "Latest Political Development"
# "Sports: Championship Results"
# ...
```

---

## â“ Troubleshooting

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Scraper Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```bash
# 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Redis
docker-compose exec redis redis-cli ping
# Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ: PONG

# 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸
docker-compose logs scraper_worker

# 3. ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ
docker-compose restart scraper_worker
```

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ°Ğ·Ğµ

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```bash
# 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ backend Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
curl http://localhost:8000/api/v1/health

# 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ scraper Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
docker-compose logs scraper_worker | grep "sending to backend"

# 3. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ
docker-compose exec scraper_worker python -c "
from app.tasks.scraping_tasks import scrape_all_sources
result = scrape_all_sources()
print(result)
"
```

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: News API Ğ¾ÑˆĞ¸Ğ±ĞºĞ° 401

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```bash
# API key Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¸Ğ»Ğ¸ Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½
# 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ .env
cat scraper_service/.env | grep NEWS_API_KEY

# 2. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ½Ğ° newsapi.org

# 3. ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ .env Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ
docker-compose restart scraper_worker
```

---

## âœ… Checklist Ğ—Ğ°Ğ¿ÑƒÑĞºĞ° Scraper

- [ ] Redis Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
- [ ] Backend API Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½
- [ ] Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PostgreSQL Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] RSS feeds Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ Ğ² config.py
- [ ] (ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) News API key ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½
- [ ] Scraper worker Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½
- [ ] Celery beat (scheduler) Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½
- [ ] Flower UI Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- [ ] Ğ›Ğ¾Ğ³Ğ¸ scraper Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³
- [ ] ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- [ ] API `/fresh` Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸

---

## ğŸ¯ Ğ˜Ñ‚Ğ¾Ğ³

**ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ:**
- âœ… ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚ (RSS)
- âœ… ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚ (Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ scrape)
- âœ… Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ
- âœ… ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ² backend Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ
- âœ… Retry Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…

**Ğ”Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾:**
```bash
docker-compose up -d scraper_worker
```

**ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ‡Ğ½ÑƒÑ‚ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· 30 ÑĞµĞºÑƒĞ½Ğ´ - 2 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹!** ğŸš€

---

**Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚:** NEWS_SCRAPER_GUIDE.md  
**Ğ”Ğ°Ñ‚Ğ°:** 20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ 2025  
**Ğ’ĞµÑ€ÑĞ¸Ñ:** 1.0
