"""
Text Embeddings Model

====== –ß–¢–û –¢–ê–ö–û–ï EMBEDDINGS? ======

Embeddings (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è) - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä.

–¢–µ–∫—Å—Ç ‚Üí –í–µ–∫—Ç–æ—Ä —á–∏—Å–µ–ª ‚Üí –ú–æ–∂–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏

====== –ó–ê–ß–ï–ú –ù–£–ñ–ù–´? ======

1. SEMANTIC SEARCH (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫):
   –ó–∞–ø—Ä–æ—Å: "AI breakthrough"
   –ù–∞—Ö–æ–¥–∏—Ç: "Artificial intelligence advancement" (—Ö–æ—Ç—è —Å–ª–æ–≤–∞ —Ä–∞–∑–Ω—ã–µ!)
   
2. SIMILARITY (–ø–æ—Ö–æ–∂–µ—Å—Ç—å):
   –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
   
3. CLUSTERING (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è):
   –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–∞–º
   
4. DEDUPLICATION (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è):
   –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–µ—Ä–µ–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ)

====== –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢? ======

–°—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥ (TF-IDF):
"apple" ‚Üí [0, 0, 1, 0, 0, ...]  # one-hot encoding
"orange" ‚Üí [0, 1, 0, 0, 0, ...]  # —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ä–∞–∑–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã

–ü—Ä–æ–±–ª–µ–º–∞: "apple" –∏ "orange" - –æ–±–∞ —Ñ—Ä—É–∫—Ç—ã, –Ω–æ –≤–µ–∫—Ç–æ—Ä—ã –Ω–µ –ø–æ—Ö–æ–∂–∏!

–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ (Embeddings):
"apple" ‚Üí [0.2, -0.5, 0.8, ...]   # dense vector
"orange" ‚Üí [0.3, -0.4, 0.7, ...]  # –ø–æ—Ö–æ–∂–∏–π –≤–µ–∫—Ç–æ—Ä!
"car" ‚Üí [-0.8, 0.3, -0.2, ...]    # —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–π –≤–µ–∫—Ç–æ—Ä

–ú–æ–¥–µ–ª—å –ü–û–ù–ò–ú–ê–ï–¢ —Å–º—ã—Å–ª –∏ —Å–æ–∑–¥–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤!

====== EXAMPLE ======

text1 = "AI revolution in healthcare"
text2 = "Artificial intelligence transforms medicine"
text3 = "Bitcoin price rises"

embedding1 = [0.2, -0.5, 0.8, 0.3, ...]  # 384 —á–∏—Å–ª–∞
embedding2 = [0.3, -0.4, 0.7, 0.4, ...]  # –ø–æ—Ö–æ–∂ –Ω–∞ embedding1!
embedding3 = [-0.8, 0.3, -0.2, -0.5, ...] # —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–π

similarity(embedding1, embedding2) = 0.95  # –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏!
similarity(embedding1, embedding3) = 0.12  # –Ω–µ –ø–æ—Ö–æ–∂–∏
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer, util
import torch
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import joblib

from app.config import settings
from app.preprocessing.text_cleaner import preprocess_for_bert


# ===== –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° =====

class TextEmbeddingModel:
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings —Ç–µ–∫—Å—Ç–∞.
    
    ====== SENTENCE-TRANSFORMERS ======
    
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É sentence-transformers - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    BERT –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings.
    
    –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏:
    
    1. all-MiniLM-L6-v2 (–Ω–∞—à default):
       - –†–∞–∑–º–µ—Ä: 80 MB
       - –°–∫–æ—Ä–æ—Å—Ç—å: –±—ã—Å—Ç—Ä–∞—è
       - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 384
       - –ö–∞—á–µ—Å—Ç–≤–æ: —Ö–æ—Ä–æ—à–µ–µ
       - –õ—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è production!
    
    2. all-mpnet-base-v2:
       - –†–∞–∑–º–µ—Ä: 420 MB
       - –°–∫–æ—Ä–æ—Å—Ç—å: –º–µ–¥–ª–µ–Ω–Ω–∞—è
       - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 768
       - –ö–∞—á–µ—Å—Ç–≤–æ: –æ—Ç–ª–∏—á–Ω–æ–µ
       - –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    
    3. multi-qa-MiniLM-L6-cos-v1:
       - –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Q&A –∏ search
       - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 384
       - –•–æ—Ä–æ—à–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
    
    ====== –ö–ê–ö –†–ê–ë–û–¢–ê–Æ–¢? ======
    
    1. BERT –∫–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç
    2. Pooling (–æ–±—ã—á–Ω–æ mean) —Å–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–∑ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤
    3. Normalization –¥–ª—è cosine similarity
    
    –ú–æ–¥–µ–ª–∏ —É–∂–µ –æ–±—É—á–µ–Ω—ã –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä —Ç–µ–∫—Å—Ç–æ–≤!
    –ù–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    def __init__(self, model_name: str = settings.EMBEDDING_MODEL):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ embeddings.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ sentence-transformers –º–æ–¥–µ–ª–∏
        """
        self.model_name = model_name
        
        print(f"üì¶ Loading embedding model: {model_name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = SentenceTransformer(model_name)
        
        # Device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        print(f"‚úÖ Model loaded (dim={self.embedding_dim}, device={self.device})")
    
    def encode(
        self,
        text: str,
        normalize: bool = True,
        convert_to_numpy: bool = True
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞—Ç—å embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä (–¥–ª—è cosine similarity)
            convert_to_numpy: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ numpy array
        
        Returns:
            –í–µ–∫—Ç–æ—Ä embeddings (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: embedding_dim)
            
        Example:
            >>> model = TextEmbeddingModel()
            >>> text = "AI revolution in healthcare"
            >>> embedding = model.encode(text)
            >>> print(embedding.shape)
            (384,)
            >>> print(embedding[:5])
            [ 0.234, -0.456,  0.789, -0.123,  0.567]
        """
        if not text:
            # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç ‚Üí –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
            return np.zeros(self.embedding_dim)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π preprocessing
        cleaned_text = preprocess_for_bert(text)
        
        # Encoding
        embedding = self.model.encode(
            cleaned_text,
            normalize_embeddings=normalize,
            convert_to_numpy=convert_to_numpy,
            show_progress_bar=False
        )
        
        return embedding
    
    def encode_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = False
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞—Ç—å embeddings –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ (–ë–´–°–¢–†–ï–ï!).
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            show_progress: –ü–æ–∫–∞–∑–∞—Ç—å progress bar
        
        Returns:
            –ú–∞—Ç—Ä–∏—Ü–∞ embeddings (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: len(texts) √ó embedding_dim)
            
        Example:
            >>> texts = ["AI news", "Sports article", "Tech update"]
            >>> embeddings = model.encode_batch(texts)
            >>> print(embeddings.shape)
            (3, 384)
        """
        # Preprocessing
        cleaned_texts = [preprocess_for_bert(text) for text in texts]
        
        # Batch encoding - –ù–ê–ú–ù–û–ì–û –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É!
        embeddings = self.model.encode(
            cleaned_texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        return embeddings
    
    def compute_similarity(
        self,
        text1: str,
        text2: str
    ) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å similarity –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç cosine similarity:
        - 1.0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ
        - 0.0 = –Ω–µ —Å–≤—è–∑–∞–Ω—ã
        - -1.0 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ (—Ä–µ–¥–∫–æ –≤ —Ç–µ–∫—Å—Ç–µ)
        
        Args:
            text1, text2: –¢–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        
        Returns:
            Similarity score (0.0 to 1.0)
            
        Example:
            >>> sim = model.compute_similarity(
            ...     "AI revolution",
            ...     "Artificial intelligence breakthrough"
            ... )
            >>> print(f"{sim:.2f}")
            0.87  # –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏!
        """
        # –°–æ–∑–¥–∞–µ–º embeddings
        emb1 = self.encode(text1)
        emb2 = self.encode(text2)
        
        # Cosine similarity
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        
        # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        # similarity = util.cos_sim(emb1, emb2).item()
        
        return float(similarity)
    
    def find_most_similar(
        self,
        query: str,
        candidates: List[str],
        top_k: int = 5
    ) -> List[Tuple[int, str, float]]:
        """–ù–∞–π—Ç–∏ —Å–∞–º—ã–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.
        
        –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ SEMANTIC SEARCH!
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            candidates: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        
        Returns:
            –°–ø–∏—Å–æ–∫ (index, text, similarity_score)
            
        Example:
            >>> query = "AI in medicine"
            >>> candidates = [
            ...     "Machine learning transforms healthcare",
            ...     "Bitcoin price rises",
            ...     "Artificial intelligence cures diseases"
            ... ]
            >>> results = model.find_most_similar(query, candidates, top_k=2)
            >>> for idx, text, score in results:
            ...     print(f"{score:.2f}: {text}")
            0.89: Machine learning transforms healthcare
            0.92: Artificial intelligence cures diseases
        """
        # Encode query
        query_emb = self.encode(query)
        
        # Encode –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (batch!)
        candidate_embs = self.encode_batch(candidates)
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity —Å query
        similarities = cosine_similarity([query_emb], candidate_embs)[0]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = [
            (int(idx), candidates[idx], float(similarities[idx]))
            for idx in top_indices
        ]
        
        return results
    
    def cluster_texts(
        self,
        texts: List[str],
        num_clusters: int = 5
    ) -> Dict[int, List[int]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Å–º—ã—Å–ª—É.
        
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã –≤–º–µ—Å—Ç–µ.
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è:
        - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞–º
        - –ù–∞—Ö–æ–∂–¥–µ–Ω–∏—è trending topics
        - –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            num_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {cluster_id: [indices —Ç–µ–∫—Å—Ç–æ–≤]}
            
        Example:
            >>> texts = [
            ...     "AI breakthrough",
            ...     "Lakers win game",
            ...     "Machine learning advances",
            ...     "NBA finals result",
            ...     "Deep learning innovation"
            ... ]
            >>> clusters = model.cluster_texts(texts, num_clusters=2)
            >>> print(clusters)
            {
                0: [0, 2, 4],  # AI topics
                1: [1, 3]      # Sports topics
            }
        """
        # –°–æ–∑–¥–∞–µ–º embeddings
        embeddings = self.encode_batch(texts)
        
        # K-Means clustering
        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        labels = kmeans.fit_predict(embeddings)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clusters = {}
        for idx, label in enumerate(labels):
            label = int(label)
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(idx)
        
        return clusters
    
    def find_duplicates(
        self,
        texts: List[str],
        threshold: float = 0.9
    ) -> List[Tuple[int, int, float]]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã/–ø–æ—á—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã.
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:
        - –û–¥–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö
        - –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π similarity –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ (index1, index2, similarity)
            
        Example:
            >>> texts = [
            ...     "Apple releases new iPhone",
            ...     "New iPhone announced by Apple",
            ...     "Bitcoin price rises"
            ... ]
            >>> dupes = model.find_duplicates(texts, threshold=0.85)
            >>> print(dupes)
            [(0, 1, 0.94)]  # –ü–µ—Ä–≤—ã–µ –¥–≤–∞ - –¥—É–±–ª–∏–∫–∞—Ç—ã!
        """
        # Embeddings
        embeddings = self.encode_batch(texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity matrix
        similarities = cosine_similarity(embeddings)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã —Å high similarity
        duplicates = []
        n = len(texts)
        
        for i in range(n):
            for j in range(i + 1, n):  # –¢–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫ –º–∞—Ç—Ä–∏—Ü—ã
                sim = similarities[i, j]
                if sim >= threshold:
                    duplicates.append((i, j, float(sim)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity (–æ—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫ –Ω–∏–∑–∫–æ–º—É)
        duplicates.sort(key=lambda x: x[2], reverse=True)
        
        return duplicates
    
    def get_centroid(self, embeddings: np.ndarray) -> np.ndarray:
        """–ü–æ–ª—É—á–∏—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥ (—Å—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä) –Ω–∞–±–æ—Ä–∞ embeddings.
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è "–ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è":
        - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —á–∏—Ç–∞–ª —Å—Ç–∞—Ç—å–∏ X, Y, Z
        - –°–æ–∑–¥–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∏—Ö embeddings
        - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º —Å—Ç–∞—Ç—å–∏ –±–ª–∏–∑–∫–∏–µ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É
        
        Args:
            embeddings: –ú–∞—Ç—Ä–∏—Ü–∞ embeddings
        
        Returns:
            –¶–µ–Ω—Ç—Ä–æ–∏–¥ (—Å—Ä–µ–¥–Ω–µ–µ)
        """
        return np.mean(embeddings, axis=0)
    
    def save_embeddings(self, embeddings: np.ndarray, path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å embeddings –Ω–∞ –¥–∏—Å–∫.
        
        Args:
            embeddings: –ú–∞—Ç—Ä–∏—Ü–∞ embeddings
            path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        np.save(path, embeddings)
        print(f"‚úÖ Embeddings saved: {path}")
    
    def load_embeddings(self, path: str) -> np.ndarray:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å embeddings —Å –¥–∏—Å–∫–∞.
        
        Args:
            path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ embeddings
        """
        embeddings = np.load(path)
        print(f"‚úÖ Embeddings loaded: {path} (shape={embeddings.shape})")
        return embeddings


# ===== UTILITY CLASS: VECTOR DATABASE (–ü—Ä–æ—Å—Ç–æ–π) =====

class SimpleVectorDB:
    """–ü—Ä–æ—Å—Ç–∞—è in-memory vector database.
    
    –î–ª—è production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:
    - Qdrant
    - Weaviate
    - Pinecone
    - Milvus
    
    –ù–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–∞ - —ç—Ç–∞ –ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è.
    """
    
    def __init__(self, embedding_model: TextEmbeddingModel):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
        """
        self.model = embedding_model
        self.embeddings = []  # –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.metadata = []    # –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç—ã, IDs, –∏ —Ç.–¥.)
    
    def add(self, text: str, metadata: Optional[Dict] = None):
        """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –≤ database.
        
        Args:
            text: –¢–µ–∫—Å—Ç
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        embedding = self.model.encode(text)
        self.embeddings.append(embedding)
        
        meta = metadata or {}
        meta['text'] = text
        self.metadata.append(meta)
    
    def add_batch(self, texts: List[str], metadatas: Optional[List[Dict]] = None):
        """–î–æ–±–∞–≤–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            metadatas: –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        embeddings = self.model.encode_batch(texts)
        self.embeddings.extend(embeddings)
        
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        for text, meta in zip(texts, metadatas):
            meta['text'] = text
            self.metadata.append(meta)
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """Semantic search –≤ database.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å metadata –∏ scores
        """
        if not self.embeddings:
            return []
        
        # Query embedding
        query_emb = self.model.encode(query)
        
        # Similarities
        embeddings_matrix = np.array(self.embeddings)
        similarities = cosine_similarity([query_emb], embeddings_matrix)[0]
        
        # Top-K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            result = self.metadata[idx].copy()
            result['score'] = float(similarities[idx])
            results.append(result)
        
        return results
    
    def save(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å database."""
        data = {
            'embeddings': np.array(self.embeddings),
            'metadata': self.metadata
        }
        joblib.dump(data, path)
    
    def load(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å database."""
        data = joblib.load(path)
        self.embeddings = list(data['embeddings'])
        self.metadata = data['metadata']


# ===== USAGE EXAMPLES =====
"""
# ===== 1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ =====

from app.models.embeddings import TextEmbeddingModel

model = TextEmbeddingModel()

# –°–æ–∑–¥–∞–µ–º embedding
text = "AI revolution in healthcare"
embedding = model.encode(text)
print(f"Embedding dimension: {embedding.shape}")
print(f"First 5 values: {embedding[:5]}")


# ===== 2. Similarity =====

text1 = "Artificial intelligence in medicine"
text2 = "AI transforms healthcare"
text3 = "Bitcoin price rises"

sim12 = model.compute_similarity(text1, text2)
sim13 = model.compute_similarity(text1, text3)

print(f"Similarity(AI, healthcare): {sim12:.2f}")  # ~0.90
print(f"Similarity(AI, bitcoin): {sim13:.2f}")     # ~0.15


# ===== 3. Semantic Search =====

query = "machine learning news"
candidates = [
    "Deep learning breakthrough in computer vision",
    "Lakers win NBA championship",
    "Neural networks advance AI research",
    "Stock market hits all-time high",
    "Artificial intelligence revolutionizes industry"
]

results = model.find_most_similar(query, candidates, top_k=3)
print("\nSearch results:")
for idx, text, score in results:
    print(f"{score:.2f}: {text}")


# ===== 4. Clustering =====

news_articles = [
    "AI breakthrough announced",
    "Machine learning improves",
    "Lakers beat Warriors",
    "NBA playoffs begin",
    "Deep learning advances",
    "Football championship final"
]

clusters = model.cluster_texts(news_articles, num_clusters=2)
print("\nClusters:")
for cluster_id, indices in clusters.items():
    print(f"Cluster {cluster_id}:")
    for idx in indices:
        print(f"  - {news_articles[idx]}")


# ===== 5. Duplicate Detection =====

articles = [
    "Apple announces new iPhone release",
    "New iPhone unveiled by Apple",
    "Tesla stock price increases",
    "Apple's latest iPhone announcement"
]

duplicates = model.find_duplicates(articles, threshold=0.80)
print("\nDuplicates found:")
for i, j, sim in duplicates:
    print(f"{sim:.2f}:")
    print(f"  [{i}] {articles[i]}")
    print(f"  [{j}] {articles[j]}")


# ===== 6. Vector Database =====

db = SimpleVectorDB(model)

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å–∏
articles = [
    "AI transforms healthcare industry",
    "Lakers win championship game",
    "New vaccine shows promising results"
]

db.add_batch(articles)

# Semantic search
query = "artificial intelligence medical"
results = db.search(query, top_k=2)

print("\nDatabase search results:")
for result in results:
    print(f"{result['score']:.2f}: {result['text']}")


# ===== 7. User Profile Recommendations =====

# –°—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—á–∏—Ç–∞–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
user_read = [
    "Machine learning advances",
    "AI in robotics",
    "Deep learning breakthrough"
]

# –°–æ–∑–¥–∞–µ–º embeddings
read_embeddings = model.encode_batch(user_read)

# –¶–µ–Ω—Ç—Ä–æ–∏–¥ = "–ø—Ä–æ—Ñ–∏–ª—å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
user_profile = model.get_centroid(read_embeddings)

# –ù–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
new_articles = [
    "Neural networks in autonomous vehicles",  # Similar to AI/ML
    "Bitcoin reaches new high",                # Different topic
    "Computer vision applications in medicine" # Similar to AI/ML
]

# –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
new_embeddings = model.encode_batch(new_articles)
similarities = cosine_similarity([user_profile], new_embeddings)[0]

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
recommendations = sorted(
    zip(new_articles, similarities),
    key=lambda x: x[1],
    reverse=True
)

print("\nRecommendations for user:")
for article, score in recommendations:
    print(f"{score:.2f}: {article}")
"""