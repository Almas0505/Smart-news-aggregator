"""
Text Classification Model

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.

====== –ß–¢–û –¢–ê–ö–û–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø? ======

–ó–∞–¥–∞—á–∞: –î–∞–Ω —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ ‚Üí –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é (Technology, Sports, –∏ —Ç.–¥.)

–ü—Ä–∏–º–µ—Ä:
Input:  "Apple releases new iPhone with AI features"
Output: "Technology" (confidence: 95%)

====== –ö–ê–ö –≠–¢–û –†–ê–ë–û–¢–ê–ï–¢? ======

1. TRAINING (–û–±—É—á–µ–Ω–∏–µ):
   - –ë–µ—Ä–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: [—Ç–µ–∫—Å—Ç, –∫–∞—Ç–µ–≥–æ—Ä–∏—è]
   - "Apple releases iPhone" ‚Üí Technology
   - "Lakers win championship" ‚Üí Sports
   
   - –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è: –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ ‚Üí –∫–∞–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
   - "iPhone", "AI", "tech" ‚Üí Technology
   - "win", "championship", "Lakers" ‚Üí Sports

2. PREDICTION (–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ):
   - –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç ‚Üí –º–æ–¥–µ–ª—å ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏—è + —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
   
====== –î–í–ê –ü–û–î–•–û–î–ê ======

–ü–æ–¥—Ö–æ–¥ 1: TF-IDF + Logistic Regression (–ø—Ä–æ—Å—Ç–æ–π, –±—ã—Å—Ç—Ä—ã–π)
–ü–æ–¥—Ö–æ–¥ 2: BERT transformer (—Ç–æ—á–Ω—ã–π, –º–µ–¥–ª–µ–Ω–Ω—ã–π)

–ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –û–ë–ê!
"""

import os
import joblib
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)

from app.config import settings
from app.preprocessing.text_cleaner import preprocess_for_tfidf, preprocess_for_bert


# ===== –ö–õ–ê–°–° 1: TF-IDF CLASSIFIER (–ü—Ä–æ—Å—Ç–æ–π) =====

class TfidfClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF + Logistic Regression.
    
    ====== –ß–¢–û –¢–ê–ö–û–ï TF-IDF? ======
    
    TF-IDF = Term Frequency - Inverse Document Frequency
    
    –ò–¥–µ—è: –í–∞–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ = –∫–∞–∫ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –í –≠–¢–û–ú –¥–æ–∫—É–º–µ–Ω—Ç–µ /
                           –∫–∞–∫ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –í–û –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    
    –ü—Ä–∏–º–µ—Ä:
    - –°–ª–æ–≤–æ "the" –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–µ–∑–¥–µ ‚Üí –Ω–∏–∑–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å (0.1)
    - –°–ª–æ–≤–æ "iPhone" –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ä–µ–¥–∫–æ ‚Üí –≤—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å (0.9)
    
    TF-IDF –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä:
    "Apple releases iPhone" ‚Üí [0.0, 0.3, 0.0, 0.9, 0.2, ...]
                               –∫–∞–∂–¥–æ–µ —á–∏—Å–ª–æ = –≤–∞–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞
    
    ====== –ß–¢–û –¢–ê–ö–û–ï LOGISTIC REGRESSION? ======
    
    –ü—Ä–æ—Å—Ç–æ–π ML –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    –£—á–∏—Ç—Å—è: –≤–µ–∫—Ç–æ—Ä —á–∏—Å–µ–ª ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    
    [0.0, 0.3, 0.0, 0.9, ...] ‚Üí "Technology"
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    + –ë—ã—Å—Ç—Ä—ã–π (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã)
    + –ü—Ä–æ—Å—Ç–æ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏
    + –ú–∞–ª–æ —Ç—Ä–µ–±—É–µ—Ç –¥–∞–Ω–Ω—ã—Ö
    
    –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
    - –ù–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç ("Apple pie" vs "Apple iPhone")
    - –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞."""
        
        # TfidfVectorizer - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞
        self.vectorizer = TfidfVectorizer(
            max_features=5000,    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-5000 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
            ngram_range=(1, 2),   # –£—á–∏—Ç—ã–≤–∞–µ–º 1-–≥—Ä–∞–º–º—ã –∏ 2-–≥—Ä–∞–º–º—ã
            # 1-–≥—Ä–∞–º–º–∞: "iPhone"
            # 2-–≥—Ä–∞–º–º–∞: "new iPhone", "releases iPhone"
            min_df=2,             # –°–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –≤ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            max_df=0.8            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ >80% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        )
        
        # LogisticRegression - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.model = LogisticRegression(
            max_iter=1000,        # –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è
            random_state=42,      # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            multi_class='multinomial',  # –î–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            solver='lbfgs'        # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        )
        
        # –ú–∞–ø–ø–∏–Ω–≥: –∏–Ω–¥–µ–∫—Å ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        self.label_to_category = {}
        self.category_to_label = {}
    
    def train(
        self,
        texts: List[str],
        labels: List[str],
        test_size: float = 0.2
    ) -> Dict[str, float]:
        """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å.
        
        ====== –ü–†–û–¶–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø ======
        
        1. –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/test (80%/20%)
        2. Preprocessing —Ç–µ–∫—Å—Ç–æ–≤
        3. TF-IDF –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (—Ç–µ–∫—Å—Ç ‚Üí —á–∏—Å–ª–∞)
        4. –û–±—É—á–µ–Ω–∏–µ Logistic Regression
        5. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            labels: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã)
            test_size: –ü—Ä–æ—Ü–µ–Ω—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏: accuracy, precision, recall, f1
            
        Example:
            texts = [
                "Apple releases new iPhone",
                "Lakers win championship"
            ]
            labels = ["technology", "sports"]
            
            classifier.train(texts, labels)
        """
        print(f"üìö –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ {len(texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
        
        # –®–∞–≥ 1: Preprocessing
        print("üßπ Preprocessing —Ç–µ–∫—Å—Ç–æ–≤...")
        cleaned_texts = [preprocess_for_tfidf(text) for text in texts]
        
        # –®–∞–≥ 2: –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        unique_labels = sorted(set(labels))
        self.label_to_category = {i: label for i, label in enumerate(unique_labels)}
        self.category_to_label = {label: i for i, label in enumerate(unique_labels)}
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ —á–∏—Å–ª–∞
        numeric_labels = [self.category_to_label[label] for label in labels]
        
        # –®–∞–≥ 3: –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            cleaned_texts,
            numeric_labels,
            test_size=test_size,
            random_state=42,
            stratify=numeric_labels  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
        )
        
        print(f"üìä Train: {len(X_train)}, Test: {len(X_test)}")
        
        # –®–∞–≥ 4: TF-IDF vectorization
        print("üî¢ TF-IDF vectorization...")
        # –û–±—É—á–∞–µ–º vectorizer –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        X_train_tfidf = self.vectorizer.fit_transform(X_train)
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ test –¥–∞–Ω–Ω—ã–º (–ë–ï–ó –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!)
        X_test_tfidf = self.vectorizer.transform(X_test)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {X_train_tfidf.shape[1]} features")
        
        # –®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üéì –û–±—É—á–∞–µ–º Logistic Regression...")
        self.model.fit(X_train_tfidf, y_train)
        
        # –®–∞–≥ 6: –û—Ü–µ–Ω–∫–∞
        print("üìà –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set...")
        y_pred = self.model.predict(X_test_tfidf)
        
        accuracy = accuracy_score(y_test, y_pred)
        print(f"‚úÖ Accuracy: {accuracy:.2%}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report = classification_report(
            y_test,
            y_pred,
            target_names=[self.label_to_category[i] for i in range(len(unique_labels))],
            output_dict=True
        )
        
        return {
            'accuracy': accuracy,
            'report': report
        }
    
    def predict(self, text: str) -> Tuple[str, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        
        ====== –ü–†–û–¶–ï–°–° –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ======
        
        1. Preprocessing —Ç–µ–∫—Å—Ç–∞
        2. TF-IDF –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        4. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        
        Args:
            text: –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
            
        Example:
            >>> text = "Apple releases new iPhone"
            >>> category, confidence = classifier.predict(text)
            >>> print(f"{category}: {confidence:.2%}")
            "Technology: 95%"
        """
        # –®–∞–≥ 1: Preprocessing
        cleaned_text = preprocess_for_tfidf(text)
        
        # –®–∞–≥ 2: TF-IDF
        text_tfidf = self.vectorizer.transform([cleaned_text])
        
        # –®–∞–≥ 3: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.model.predict(text_tfidf)[0]
        
        # –®–∞–≥ 4: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        probabilities = self.model.predict_proba(text_tfidf)[0]
        confidence = probabilities[prediction]
        
        # –®–∞–≥ 5: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category = self.label_to_category[prediction]
        
        return category, confidence
    
    def predict_batch(self, texts: List[str]) -> List[Tuple[str, float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        –ë—ã—Å—Ç—Ä–µ–µ, —á–µ–º –≤—ã–∑—ã–≤–∞—Ç—å predict() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        """
        # Preprocessing –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
        cleaned_texts = [preprocess_for_tfidf(text) for text in texts]
        
        # TF-IDF
        texts_tfidf = self.vectorizer.transform(cleaned_texts)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.model.predict(texts_tfidf)
        probabilities = self.model.predict_proba(texts_tfidf)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for pred, probs in zip(predictions, probabilities):
            category = self.label_to_category[pred]
            confidence = probs[pred]
            results.append((category, confidence))
        
        return results
    
    def save(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –¥–∏—Å–∫.
        
        Args:
            path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –≤–º–µ—Å—Ç–µ
        model_data = {
            'vectorizer': self.vectorizer,
            'model': self.model,
            'label_to_category': self.label_to_category,
            'category_to_label': self.category_to_label
        }
        
        joblib.dump(model_data, path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")
    
    def load(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å –¥–∏—Å–∫–∞.
        
        Args:
            path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        """
        model_data = joblib.load(path)
        
        self.vectorizer = model_data['vectorizer']
        self.model = model_data['model']
        self.label_to_category = model_data['label_to_category']
        self.category_to_label = model_data['category_to_label']
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {path}")


# ===== –ö–õ–ê–°–° 2: BERT CLASSIFIER (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π) =====

class BertClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT transformer.
    
    ====== –ß–¢–û –¢–ê–ö–û–ï BERT? ======
    
    BERT = Bidirectional Encoder Representations from Transformers
    
    –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google (2018).
    
    –û—Ç–ª–∏—á–∏—è –æ—Ç TF-IDF:
    1. –ü–æ–Ω–∏–º–∞–µ—Ç –ö–û–ù–¢–ï–ö–°–¢
       - "Apple pie" vs "Apple iPhone" - —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è!
       - TF-IDF: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
       - BERT: —Ä–∞–∑–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
    
    2. –ü–æ–Ω–∏–º–∞–µ—Ç –ü–û–†–Ø–î–û–ö –°–õ–û–í
       - "Dog bites man" vs "Man bites dog" - —Ä–∞–∑–Ω—ã–π —Å–º—ã—Å–ª!
       - TF-IDF: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
       - BERT: —Ä–∞–∑–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
    
    3. PRETRAINED (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è)
       - –û–±—É—á–µ–Ω–∞ –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤
       - –£–∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫
       - –ú—ã —Ç–æ–ª—å–∫–æ "fine-tune" (–¥–æ–æ–±—É—á–∞–µ–º) –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    + –û—á–µ–Ω—å —Ç–æ—á–Ω–∞—è (90-95%+ accuracy)
    + –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
    + –¢—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö (transfer learning)
    
    –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
    - –ú–µ–¥–ª–µ–Ω–Ω–∞—è (—Å–µ–∫—É–Ω–¥—ã –≤–º–µ—Å—Ç–æ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥)
    - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏
    - –°–ª–æ–∂–Ω–µ–µ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏
    """
    
    def __init__(self, model_name: str = settings.CLASSIFICATION_MODEL):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            model_name: –ò–º—è pretrained –º–æ–¥–µ–ª–∏
                       "distilbert-base-uncased" - –±—ã—Å—Ç—Ä–∞—è
                       "bert-base-uncased" - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è
                       "roberta-base" - —É–ª—É—á—à–µ–Ω–Ω–∞—è
        """
        self.model_name = model_name
        self.tokenizer = None  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        self.model = None
        self.label_to_category = {}
        self.category_to_label = {}
        
        # Device (CPU –∏–ª–∏ GPU)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è  Using device: {self.device}")
    
    # TODO: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è train, predict –¥–ª—è BERT
    # –≠—Ç–æ –±—É–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–∞–π–ª–µ, —Ç.–∫. BERT –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π


# ===== USAGE EXAMPLES =====
"""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

# ===== TF-IDF Classifier =====

# 1. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ
classifier = TfidfClassifier()

texts = [
    "Apple releases new iPhone with AI features",
    "Tesla announces electric car breakthrough",
    "Lakers win NBA championship",
    "New COVID vaccine shows promising results"
]

labels = [
    "technology",
    "technology",
    "sports",
    "health"
]

metrics = classifier.train(texts, labels)
print(f"Accuracy: {metrics['accuracy']:.2%}")

# 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
text = "Google launches new AI product"
category, confidence = classifier.predict(text)
print(f"Category: {category} ({confidence:.2%})")

# 3. Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
texts = [
    "Bitcoin price rises",
    "Olympics begin in Paris"
]
results = classifier.predict_batch(texts)
for text, (cat, conf) in zip(texts, results):
    print(f"{text[:30]}: {cat} ({conf:.2%})")

# 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞
classifier.save("./models/classifier.joblib")
classifier.load("./models/classifier.joblib")


# ===== BERT Classifier (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω) =====
# bert_classifier = BertClassifier()
# bert_classifier.train(texts, labels)
# category, confidence = bert_classifier.predict(text)
"""