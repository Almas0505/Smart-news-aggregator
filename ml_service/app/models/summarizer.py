"""
Text Summarization Model

====== –ß–¢–û –¢–ê–ö–û–ï SUMMARIZATION? ======

–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–ó–∞–¥–∞—á–∞: –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (500 —Å–ª–æ–≤) ‚Üí –ö–æ—Ä–æ—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (50 —Å–ª–æ–≤)

====== –ó–ê–ß–ï–ú –ù–£–ñ–ù–û? ======

1. –ü—Ä–µ–≤—å—é –Ω–æ–≤–æ—Å—Ç–µ–π: –ø–æ–∫–∞–∑–∞—Ç—å —Å—É—Ç—å –±–µ–∑ —á—Ç–µ–Ω–∏—è –≤—Å–µ–≥–æ
2. –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏: –±—ã—Å—Ç—Ä–æ –ø–æ–Ω—è—Ç—å –æ —á–µ–º —Å—Ç–∞—Ç—å—è
3. –ú–æ–±–∏–ª—å–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –ª—É—á—à–µ —á–∏—Ç–∞–µ—Ç—Å—è
4. SEO: meta descriptions –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤

====== –î–í–ê –ü–û–î–•–û–î–ê ======

1. EXTRACTIVE (–∏–∑–≤–ª–µ–∫–∞—é—â–∞—è):
   - –í—ã–±–∏—Ä–∞–µ—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –∏–∑ —Ç–µ–∫—Å—Ç–∞
   - –ü—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç –∏—Ö –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
   - –ë—ã—Å—Ç—Ä–æ, –Ω–æ –º–µ–Ω–µ–µ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ
   
   –ü—Ä–∏–º–µ—Ä:
   Original: "The cat sat. It was black. The dog ran. It was fast."
   Summary: "The cat sat. The dog ran."

2. ABSTRACTIVE (–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–∞—è):
   - –ì–ï–ù–ï–†–ò–†–£–ï–¢ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç
   - –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
   - –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ
   
   –ü—Ä–∏–º–µ—Ä:
   Original: "The cat sat. It was black. The dog ran. It was fast."
   Summary: "A black cat and fast dog were active."

–ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –û–ë–ê –ø–æ–¥—Ö–æ–¥–∞!
"""

import numpy as np
from typing import List, Dict, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import networkx as nx

from app.config import settings
from app.preprocessing.text_cleaner import preprocess_text, remove_html_tags


# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


# ===== EXTRACTIVE SUMMARIZATION =====

class ExtractiveSummarizer:
    """Extractive —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    
    ====== –ê–õ–ì–û–†–ò–¢–ú: TextRank ======
    
    TextRank - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PageRank –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤.
    
    –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç PageRank:
    - –í–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã = —É–∑–ª—ã –≥—Ä–∞—Ñ–∞
    - –°—Å—ã–ª–∫–∏ = —Ä–µ–±—Ä–∞ –≥—Ä–∞—Ñ–∞
    - –í–∞–∂–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã = —Å–∫–æ–ª—å–∫–æ –Ω–∞ –Ω–µ–µ —Å—Å—ã–ª–æ–∫
    
    –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç TextRank:
    - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è = —É–∑–ª—ã –≥—Ä–∞—Ñ–∞
    - –ü–æ—Ö–æ–∂–µ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π = —Ä–µ–±—Ä–∞ –≥—Ä–∞—Ñ–∞
    - –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è = –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–æ —Å–≤—è–∑–∞–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏
    
    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    2. –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (TF-IDF)
    3. –°—á–∏—Ç–∞–µ–º similarity –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    4. –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ: —É–∑–ª—ã = –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ä–µ–±—Ä–∞ = similarity
    5. –ü—Ä–∏–º–µ–Ω—è–µ–º PageRank ‚Üí –ø–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    6. –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    + –ë—ã—Å—Ç—Ä–æ (—Å–µ–∫—É–Ω–¥—ã)
    + –ù–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è
    + –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    + –ü–æ–Ω—è—Ç–Ω–æ –∏ –æ–±—ä—è—Å–Ω–∏–º–æ
    
    –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
    - –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—á–∏—Ç–∞–±–µ–ª—å–Ω–æ (–ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    - –ù–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç
    - –ú–æ–∂–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è."""
        self.vectorizer = TfidfVectorizer()
    
    def _sentence_similarity(self, sent1: str, sent2: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –ø–æ—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ–º cosine similarity –º–µ–∂–¥—É TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–º–∏.
        
        Args:
            sent1, sent2: –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Returns:
            Similarity score (0.0 –¥–æ 1.0)
        """
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –æ–±–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        try:
            vectors = self.vectorizer.fit_transform([sent1, sent2])
            
            # Cosine similarity
            similarity = (vectors * vectors.T).toarray()[0, 1]
            
            return similarity
        except:
            return 0.0
    
    def summarize(
        self,
        text: str,
        num_sentences: int = 3,
        min_sentence_length: int = 10
    ) -> str:
        """–°–æ–∑–¥–∞—Ç—å extractive summary.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            num_sentences: –°–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–µ–∑—é–º–µ
            min_sentence_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–≤ —Å–ª–æ–≤–∞—Ö)
        
        Returns:
            Summary
            
        Example:
            >>> summarizer = ExtractiveSummarizer()
            >>> text = "Long article text here... (many sentences)"
            >>> summary = summarizer.summarize(text, num_sentences=3)
            >>> print(summary)
        """
        if not text:
            return ""
        
        # –û—á–∏—Å—Ç–∫–∞ HTML
        text = remove_html_tags(text)
        
        # –®–∞–≥ 1: –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = sent_tokenize(text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = [
            sent for sent in sentences
            if len(sent.split()) >= min_sentence_length
        ]
        
        if len(sentences) <= num_sentences:
            # –¢–µ–∫—Å—Ç —É–∂–µ –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return ' '.join(sentences)
        
        # –®–∞–≥ 2: Preprocessing –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ª–µ–≥–∫–∏–π)
        cleaned_sentences = [
            preprocess_text(
                sent,
                remove_html=False,  # —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ
                remove_stops=False,  # –Ω—É–∂–Ω—ã –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
                lemmatize=False
            )
            for sent in sentences
        ]
        
        # –®–∞–≥ 3: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ TF-IDF
        try:
            sentence_vectors = self.vectorizer.fit_transform(cleaned_sentences)
        except:
            # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            return ' '.join(sentences[:num_sentences])
        
        # –®–∞–≥ 4: –ú–∞—Ç—Ä–∏—Ü–∞ similarity
        # similarity_matrix[i][j] = –ø–æ—Ö–æ–∂–µ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è i –∏ j
        similarity_matrix = (sentence_vectors * sentence_vectors.T).toarray()
        
        # –®–∞–≥ 5: –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ
        nx_graph = nx.from_numpy_array(similarity_matrix)
        
        # –®–∞–≥ 6: PageRank –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        scores = nx.pagerank(nx_graph)
        
        # –®–∞–≥ 7: –†–∞–Ω–∂–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        ranked_sentences = sorted(
            ((scores[i], i, sent) for i, sent in enumerate(sentences)),
            reverse=True
        )
        
        # –®–∞–≥ 8: –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        top_sentences = ranked_sentences[:num_sentences]
        
        # –®–∞–≥ 9: –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É (–¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏)
        top_sentences = sorted(top_sentences, key=lambda x: x[1])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º summary
        summary = ' '.join([sent for _, _, sent in top_sentences])
        
        return summary
    
    def get_sentence_scores(self, text: str) -> List[Dict[str, any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ debugging.
        
        Args:
            text: –¢–µ–∫—Å—Ç
        
        Returns:
            –°–ø–∏—Å–æ–∫ {sentence, score, rank}
        """
        sentences = sent_tokenize(remove_html_tags(text))
        cleaned = [preprocess_text(s, remove_stops=False) for s in sentences]
        
        try:
            vectors = self.vectorizer.fit_transform(cleaned)
            similarity_matrix = (vectors * vectors.T).toarray()
            graph = nx.from_numpy_array(similarity_matrix)
            scores = nx.pagerank(graph)
            
            results = []
            for i, (sent, score) in enumerate(zip(sentences, scores.values())):
                results.append({
                    "sentence": sent,
                    "score": score,
                    "rank": i + 1
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x["score"], reverse=True)
            
            return results
        except:
            return []


# ===== ABSTRACTIVE SUMMARIZATION =====

class AbstractiveSummarizer:
    """Abstractive —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç.
    
    ====== –ú–û–î–ï–õ–ò ======
    
    –ò—Å–ø–æ–ª—å–∑—É–µ–º SOTA transformer –º–æ–¥–µ–ª–∏:
    
    1. BART (Facebook):
       - facebook/bart-large-cnn
       - –û–±—É—á–µ–Ω–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—è—Ö CNN/DailyMail
       - –û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è news summarization
    
    2. Pegasus (Google):
       - google/pegasus-xsum
       - –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è extreme summarization
       - –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ summaries
    
    3. T5 (Google):
       - t5-base, t5-large
       - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    
    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º BART - –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π.
    
    ====== –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢? ======
    
    1. Encoder —á–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    2. –°–æ–∑–¥–∞–µ—Ç internal representation (–ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞)
    3. Decoder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary —Å–ª–æ–≤–æ –∑–∞ —Å–ª–æ–≤–æ–º
    4. –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    –≠—Ç–æ seq2seq –º–æ–¥–µ–ª—å (sequence to sequence):
    Input sequence (long text) ‚Üí Output sequence (short summary)
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    + –°–æ–∑–¥–∞–µ—Ç —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    + –ú–æ–∂–µ—Ç –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å
    + –ü–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª
    
    –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
    - –ú–µ–¥–ª–µ–Ω–Ω–æ (5-10 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Ç–µ–∫—Å—Ç)
    - –¢—Ä–µ–±—É–µ—Ç GPU –¥–ª—è —Ö–æ—Ä–æ—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    - –ú–æ–∂–µ—Ç "–≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞—Ç—å" (–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã)
    """
    
    def __init__(
        self,
        model_name: str = settings.SUMMARIZATION_MODEL
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            model_name: Hugging Face –º–æ–¥–µ–ª—å
        """
        self.model_name = model_name
        
        print(f"üì¶ Loading summarization model: {model_name}")
        
        # Pipeline –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        self.pipeline = pipeline(
            "summarization",
            model=model_name,
            device=-1  # CPU (-1) –∏–ª–∏ GPU (0)
        )
        
        # Tokenizer –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print(f"‚úÖ Model loaded successfully")
    
    def summarize(
        self,
        text: str,
        max_length: int = 130,
        min_length: int = 30,
        do_sample: bool = False
    ) -> str:
        """–°–æ–∑–¥–∞—Ç—å abstractive summary.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            max_length: –ú–∞–∫—Å –¥–ª–∏–Ω–∞ summary (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            min_length: –ú–∏–Ω –¥–ª–∏–Ω–∞ summary (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            do_sample: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sampling (–±–æ–ª–µ–µ creative, –Ω–æ –º–µ–Ω–µ–µ stable)
        
        Returns:
            Summary
            
        Example:
            >>> summarizer = AbstractiveSummarizer()
            >>> text = "Long news article..."
            >>> summary = summarizer.summarize(text, max_length=50)
            >>> print(summary)
            "Short generated summary of the article."
        """
        if not text:
            return ""
        
        # –û—á–∏—Å—Ç–∫–∞
        text = remove_html_tags(text)
        
        # BART/Pegasus –∏–º–µ—é—Ç –ª–∏–º–∏—Ç ~1024 —Ç–æ–∫–µ–Ω–∞
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ, –æ–±—Ä–µ–∑–∞–µ–º
        tokens = self.tokenizer.encode(text, truncation=True, max_length=1024)
        text = self.tokenizer.decode(tokens, skip_special_tokens=True)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è summary
        try:
            result = self.pipeline(
                text,
                max_length=max_length,
                min_length=min_length,
                do_sample=do_sample,
                truncation=True
            )
            
            summary = result[0]['summary_text']
            
            return summary
            
        except Exception as e:
            print(f"‚ùå Summarization failed: {e}")
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            sentences = sent_tokenize(text)
            return ' '.join(sentences[:2])
    
    def summarize_batch(
        self,
        texts: List[str],
        max_length: int = 130,
        min_length: int = 30,
        batch_size: int = 4
    ) -> List[str]:
        """Batch —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è (–±—ã—Å—Ç—Ä–µ–µ –¥–ª—è –º–Ω–æ–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤).
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            max_length: –ú–∞–∫—Å –¥–ª–∏–Ω–∞
            min_length: –ú–∏–Ω –¥–ª–∏–Ω–∞
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ summaries
        """
        # Preprocessing
        cleaned_texts = [remove_html_tags(text) for text in texts]
        
        # Truncate
        truncated_texts = []
        for text in cleaned_texts:
            tokens = self.tokenizer.encode(text, truncation=True, max_length=1024)
            truncated_texts.append(
                self.tokenizer.decode(tokens, skip_special_tokens=True)
            )
        
        # Batch generation
        try:
            results = self.pipeline(
                truncated_texts,
                max_length=max_length,
                min_length=min_length,
                batch_size=batch_size,
                truncation=True
            )
            
            summaries = [r['summary_text'] for r in results]
            return summaries
            
        except Exception as e:
            print(f"‚ùå Batch summarization failed: {e}")
            # Fallback
            return [' '.join(sent_tokenize(t)[:2]) for t in truncated_texts]


# ===== HYBRID SUMMARIZER =====

class HybridSummarizer:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç extractive –∏ abstractive.
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. Extractive: –≤—ã–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–±—ã—Å—Ç—Ä–æ)
    2. Abstractive: –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ–º –∏—Ö (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ)
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    + –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º pure abstractive
    + –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ —á–µ–º pure extractive
    + –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è."""
        self.extractive = ExtractiveSummarizer()
        self.abstractive = AbstractiveSummarizer()
    
    def summarize(
        self,
        text: str,
        num_sentences_extract: int = 5,
        max_length_abstract: int = 100
    ) -> str:
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            num_sentences_extract: –°–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑–≤–ª–µ—á—å
            max_length_abstract: –ú–∞–∫—Å –¥–ª–∏–Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ summary
        
        Returns:
            Summary
        """
        # –®–∞–≥ 1: Extractive (–≤—ã–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω—ã–µ —á–∞—Å—Ç–∏)
        extracted = self.extractive.summarize(text, num_sentences=num_sentences_extract)
        
        # –®–∞–≥ 2: Abstractive (–ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ–º)
        final_summary = self.abstractive.summarize(
            extracted,
            max_length=max_length_abstract
        )
        
        return final_summary


# ===== UTILITY FUNCTIONS =====

def calculate_compression_ratio(original: str, summary: str) -> float:
    """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è.
    
    Args:
        original: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        summary: Summary
    
    Returns:
        Compression ratio (0.0 to 1.0)
        
    Example:
        >>> ratio = calculate_compression_ratio("100 words", "20 words")
        >>> print(ratio)
        0.2  # Summary is 20% of original
    """
    orig_words = len(original.split())
    summ_words = len(summary.split())
    
    if orig_words == 0:
        return 0.0
    
    return summ_words / orig_words


def get_summary_stats(original: str, summary: str) -> Dict[str, any]:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ summary.
    
    Args:
        original: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        summary: Summary
    
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    """
    return {
        "original_length": len(original),
        "original_words": len(original.split()),
        "original_sentences": len(sent_tokenize(original)),
        "summary_length": len(summary),
        "summary_words": len(summary.split()),
        "summary_sentences": len(sent_tokenize(summary)),
        "compression_ratio": calculate_compression_ratio(original, summary)
    }


# ===== USAGE EXAMPLES =====
"""
# ===== 1. Extractive Summarization =====

extractive = ExtractiveSummarizer()

text = '''
Long news article about AI breakthrough.
Multiple sentences with different information.
Some sentences are more important than others.
The key findings are in certain sentences.
Other sentences provide background context.
'''

# –°–æ–∑–¥–∞–µ–º summary –∏–∑ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
summary = extractive.summarize(text, num_sentences=2)
print(summary)

# –°–º–æ—Ç—Ä–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
scores = extractive.get_sentence_scores(text)
for item in scores[:3]:  # —Ç–æ–ø-3
    print(f"Score {item['score']:.3f}: {item['sentence'][:50]}...")


# ===== 2. Abstractive Summarization =====

abstractive = AbstractiveSummarizer()

text = '''
Apple Inc. announced today the release of the new iPhone 15 
with revolutionary AI features. The device features an advanced 
neural engine capable of real-time language translation and 
enhanced photography. CEO Tim Cook stated that this represents 
a major leap forward in mobile technology.
'''

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary
summary = abstractive.summarize(text, max_length=50)
print(summary)
# Output (generated): "Apple releases iPhone 15 with AI features, 
#                     neural engine for translation and photography."


# Batch summarization
texts = [
    "Long article 1...",
    "Long article 2...",
    "Long article 3..."
]
summaries = abstractive.summarize_batch(texts)
for text, summ in zip(texts, summaries):
    print(f"Original: {text[:50]}...")
    print(f"Summary:  {summ}\n")


# ===== 3. Hybrid Summarization =====

hybrid = HybridSummarizer()

text = "Very long news article with many details..."
summary = hybrid.summarize(text)
print(summary)


# ===== 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ =====

original = "Long article text..."
summary = "Short summary..."

stats = get_summary_stats(original, summary)
print(f"Compression: {stats['compression_ratio']:.1%}")
print(f"Original: {stats['original_words']} words ‚Üí Summary: {stats['summary_words']} words")


# ===== –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥–∞ =====

# –î–ª—è –°–ö–û–†–û–°–¢–ò (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã):
summarizer = ExtractiveSummarizer()

# –î–ª—è –ö–ê–ß–ï–°–¢–í–ê (—Å–µ–∫—É–Ω–¥—ã):
summarizer = AbstractiveSummarizer()

# –î–ª—è –ë–ê–õ–ê–ù–°–ê:
summarizer = HybridSummarizer()
"""